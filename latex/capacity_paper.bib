
@article{donoho_precise_2010,
	title = {Precise {Undersampling} {Theorems}},
	volume = {98},
	issn = {1558-2256},
	url = {https://ieeexplore.ieee.org/document/5458001},
	doi = {10.1109/JPROC.2010.2045630},
	abstract = {Undersampling theorems state that we may gather far fewer samples than the usual sampling theorem while exactly reconstructing the object of interest-provided the object in question obeys a sparsity condition, the samples measure appropriate linear combinations of signal values, and we reconstruct with a particular nonlinear procedure. While there are many ways to crudely demonstrate such undersampling phenomena, we know of only one mathematically rigorous approach which precisely quantifies the true sparsity-undersampling tradeoff curve of standard algorithms and standard compressed sensing matrices. That approach, based on combinatorial geometry, predicts the exact location in sparsity-undersampling domain where standard algorithms exhibit phase transitions in performance. We review the phase transition approach here and describe the broad range of cases where it applies. We also mention exceptions and state challenge problems for future research. Sample result: one can efficiently reconstruct a k-sparse signal of length N from n measurements, provided n ¿ 2k · log(N/n), for (k,n,N) large.k ¿ N.AMS 2000 subject classifications . Primary: 41A46, 52A22, 52B05, 62E20, 68P30, 94A20; Secondary: 15A52, 60F10, 68P25, 90C25, 94B20.},
	number = {6},
	urldate = {2025-07-29},
	journal = {Proceedings of the IEEE},
	author = {Donoho, David L. and Tanner, Jared},
	month = jun,
	year = {2010},
	keywords = {{\textbackslash}ell₁-minimization, Bandlimited measurements, compressed sensing, Compressed sensing, Geometry, Heart, Image reconstruction, Image sampling, Length measurement, Magnetic resonance imaging, Particle measurements, Psychology, random measurements, random polytopes, Sampling methods, superresolution, undersampling, universality of matrix ensembles},
	pages = {913--924},
	file = {Accepted Version:/Users/cgadski/Zotero/storage/JW7U2IZH/Donoho and Tanner - 2010 - Precise Undersampling Theorems.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/GC6CHNK3/5458001.html:text/html},
}

@article{tropp_signal_2007,
	title = {Signal {Recovery} {From} {Random} {Measurements} {Via} {Orthogonal} {Matching} {Pursuit}},
	volume = {53},
	issn = {1557-9654},
	url = {https://ieeexplore.ieee.org/document/4385788},
	doi = {10.1109/TIT.2007.909108},
	abstract = {This paper demonstrates theoretically and empirically that a greedy algorithm called Orthogonal Matching Pursuit (OMP) can reliably recover a signal with m nonzero entries in dimension d given {\textbackslash}rm O(m łn d) random linear measurements of that signal. This is a massive improvement over previous results, which require {\textbackslash}rm O(m{\textasciicircum}2) measurements. The new results for OMP are comparable with recent results for another approach called Basis Pursuit (BP). In some settings, the OMP algorithm is faster and easier to implement, so it is an attractive alternative to BP for signal recovery problems.},
	number = {12},
	urldate = {2025-07-29},
	journal = {IEEE Transactions on Information Theory},
	author = {Tropp, Joel A. and Gilbert, Anna C.},
	month = dec,
	year = {2007},
	keywords = {Mathematics, compressed sensing, Compressed sensing, Algorithms, approximation, basis pursuit, Blood, Greedy algorithms, group testing, Matching pursuit algorithms, orthogonal matching pursuit, Performance evaluation, Reliability theory, Signal processing, signal recovery, sparse approximation, Testing, Vectors},
	pages = {4655--4666},
	file = {Accepted Version:/Users/cgadski/Zotero/storage/VYM2K44W/Tropp and Gilbert - 2007 - Signal Recovery From Random Measurements Via Ortho.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/ECCCLVLZ/4385788.html:text/html},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	issn = {1533-7928},
	shorttitle = {Scikit-learn},
	url = {http://jmlr.org/papers/v12/pedregosa11a.html},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	number = {85},
	urldate = {2025-01-29},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Edouard},
	year = {2011},
	pages = {2825--2830},
	file = {Full Text PDF:/Users/cgadski/Zotero/storage/2ISD5GMY/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf:application/pdf},
}

@misc{bussmann_learning_2025,
	title = {Learning {Multi}-{Level} {Features} with {Matryoshka} {Sparse} {Autoencoders}},
	url = {http://arxiv.org/abs/2503.17547},
	doi = {10.48550/arXiv.2503.17547},
	abstract = {Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting neural networks by extracting the concepts represented in their activations. However, choosing the size of the SAE dictionary (i.e. number of learned concepts) creates a tension: as dictionary size increases to capture more relevant concepts, sparsity incentivizes features to be split or absorbed into more specific features, leaving high-level features missing or warped. We introduce Matryoshka SAEs, a novel variant that addresses these issues by simultaneously training multiple nested dictionaries of increasing size, forcing the smaller dictionaries to independently reconstruct the inputs without using the larger dictionaries. This organizes features hierarchically - the smaller dictionaries learn general concepts, while the larger dictionaries learn more specific concepts, without incentive to absorb the high-level features. We train Matryoshka SAEs on Gemma-2-2B and TinyStories and find superior performance on sparse probing and targeted concept erasure tasks, more disentangled concept representations, and reduced feature absorption. While there is a minor tradeoff with reconstruction performance, we believe Matryoshka SAEs are a superior alternative for practical tasks, as they enable training arbitrarily large SAEs while retaining interpretable features at different levels of abstraction.},
	urldate = {2025-05-16},
	publisher = {arXiv},
	author = {Bussmann, Bart and Nabeshima, Noa and Karvonen, Adam and Nanda, Neel},
	month = mar,
	year = {2025},
	note = {arXiv:2503.17547 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/R36B5YTY/Bussmann et al. - 2025 - Learning Multi-Level Features with Matryoshka Spar.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/CX8JZMJT/2503.html:text/html},
}

@misc{gurnee_language_2024,
	title = {Language {Models} {Represent} {Space} and {Time}},
	url = {http://arxiv.org/abs/2310.02207},
	doi = {10.48550/arXiv.2310.02207},
	abstract = {The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual "space neurons" and "time neurons" that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model.},
	urldate = {2025-05-15},
	publisher = {arXiv},
	author = {Gurnee, Wes and Tegmark, Max},
	month = mar,
	year = {2024},
	note = {arXiv:2310.02207 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/I7A9V63T/Gurnee and Tegmark - 2024 - Language Models Represent Space and Time.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/79QA49AD/2310.html:text/html},
}

@article{baraniuk_simple_2008,
	title = {A {Simple} {Proof} of the {Restricted} {Isometry} {Property} for {Random} {Matrices}},
	volume = {28},
	issn = {1432-0940},
	url = {https://doi.org/10.1007/s00365-007-9003-x},
	doi = {10.1007/s00365-007-9003-x},
	abstract = {We give a simple technique for verifying the Restricted Isometry Property (as introduced by Candès and Tao) for random matrices that underlies Compressed Sensing. Our approach has two main ingredients: (i) concentration inequalities for random inner products that have recently provided algorithmically simple proofs of the Johnson–Lindenstrauss lemma; and (ii) covering numbers for finite-dimensional balls in Euclidean space. This leads to an elementary proof of the Restricted Isometry Property and brings out connections between Compressed Sensing and the Johnson–Lindenstrauss lemma. As a result, we obtain simple and direct proofs of Kashin’s theorems on widths of finite balls in Euclidean space (and their improvements due to Gluskin) and proofs of the existence of optimal Compressed Sensing measurement matrices. In the process, we also prove that these measurements have a certain universality with respect to the sparsity-inducing basis.},
	language = {en},
	number = {3},
	urldate = {2025-03-25},
	journal = {Constructive Approximation},
	author = {Baraniuk, Richard and Davenport, Mark and DeVore, Ronald and Wakin, Michael},
	month = dec,
	year = {2008},
	keywords = {Compressed sensing, 15A52, 15N2, 60F10, 94A12, 94A20, Concentration inequalities, Random matrices, Sampling},
	pages = {253--263},
	file = {Full Text PDF:/Users/cgadski/Zotero/storage/UQ3Z32MF/Baraniuk et al. - 2008 - A Simple Proof of the Restricted Isometry Property.pdf:application/pdf},
}

@article{candes_restricted_2008,
	title = {The restricted isometry property and its implications for compressed sensing},
	volume = {346},
	issn = {1631-073X},
	url = {https://www.sciencedirect.com/science/article/pii/S1631073X08000964},
	doi = {10.1016/j.crma.2008.03.014},
	abstract = {It is now well-known that one can reconstruct sparse or compressible signals accurately from a very limited number of measurements, possibly contaminated with noise. This technique known as “compressed sensing” or “compressive sampling” relies on properties of the sensing matrix such as the restricted isometry property. In this Note, we establish new results about the accuracy of the reconstruction from undersampled measurements which improve on earlier estimates, and have the advantage of being more elegant. To cite this article: E.J. Candès, C. R. Acad. Sci. Paris, Ser. I 346 (2008).
Résumé
Il est maintenant bien connu que l'on peut reconstruire des signaux compressibles de manière précise à partir d'un nombre étonnamment petit de mesures, peut-être même bruitées. Cette technique appelée le “compressed sensing” ou “compressive sampling” utilise des propriétés de la matrice d'échantillonage comme la propriété d'isométrie restreinte. Dans cette Note, nous présentons de nouveaux résultats sur la reconstruction de signaux à partir de données incomplètes qui améliorent des travaux précedents et qui, en outre, ont l'avantage d'être plus élégants. Pour citer cet article : E.J. Candès, C. R. Acad. Sci. Paris, Ser. I 346 (2008).},
	number = {9},
	urldate = {2025-03-25},
	journal = {Comptes Rendus Mathematique},
	author = {Candès, Emmanuel J.},
	month = may,
	year = {2008},
	pages = {589--592},
	file = {Full Text:/Users/cgadski/Zotero/storage/PPISNBVD/Candès - 2008 - The restricted isometry property and its implicati.pdf:application/pdf;ScienceDirect Snapshot:/Users/cgadski/Zotero/storage/AGD8A6NP/S1631073X08000964.html:text/html},
}

@article{cahill_gap_2016,
	title = {The gap between the null space property and the restricted isometry property},
	volume = {501},
	issn = {0024-3795},
	url = {https://www.sciencedirect.com/science/article/pii/S0024379516300088},
	doi = {10.1016/j.laa.2016.03.022},
	abstract = {The null space property (NSP) and the restricted isometry property (RIP) are two properties which have received considerable attention in the compressed sensing literature. As the name suggests, NSP is a property that depends solely on the null space of the measurement procedure and as such, any two matrices which have the same null space will have NSP if either one of them does. On the other hand, RIP is a property of the measurement procedure itself, and given an RIP matrix it is straightforward to construct another matrix with the same null space that is not RIP. We say a matrix is RIP-NSP if it has the same null space as an RIP matrix. We show that such matrices can provide robust recovery of compressible signals under Basis pursuit. More importantly, we constructively show that the RIP-NSP is stronger than NSP with the aid of this robust recovery result, which shows that RIP is fundamentally stronger than NSP.},
	urldate = {2025-03-25},
	journal = {Linear Algebra and its Applications},
	author = {Cahill, Jameson and Chen, Xuemei and Wang, Rongrong},
	month = jul,
	year = {2016},
	keywords = {Compressed sensing, RIP-NSP, Sparse representation, Stability},
	pages = {363--375},
	file = {ScienceDirect Snapshot:/Users/cgadski/Zotero/storage/9D5W5ZPF/S0024379516300088.html:text/html;Submitted Version:/Users/cgadski/Zotero/storage/C2YDTP6Q/Cahill et al. - 2016 - The gap between the null space property and the re.pdf:application/pdf},
}

@article{gadzinski_bitrates_2025,
	title = {On {Bitrates} of {Very} {Sparse} {Superposition} {Codes}},
	url = {https://openreview.net/forum?id=peR9HAdJnk&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICML.cc%2F2025%2FConference%2FAuthors%23your-submissions)},
	abstract = {\_Sparse autoencoders\_ have been used to interpret activity inside large language models as "superposition codes" for sparse, high-dimensional signals. The encoder layers of these autoencoders use simple methods, which we will call "one-step estimates," to read latent sparse signals from vectors of hidden neuron activations. This work investigates the reliability of one-step estimates on a generic family of sparse inference problems. We show that these estimates are remarkably inefficient from the point of view of coding theory: even in a "very sparse" regime, they are only reliable when the dimension of the code exceeds the entropy of the latent signal by a factor of \$2.7\$ dimensions per bit. In comparison, a very naive iterative method called matching pursuit can read superposition codes given just \$1.3\$ dimensions per bit. This opens the question of whether neural networks can achieve similar bitrates in their internal representations.},
	language = {en},
	urldate = {2025-03-20},
	author = {Gadzinski, Christopher Neil and Mocanu, Decebal Constantin},
	month = jan,
	year = {2025},
	file = {Full Text PDF:/Users/cgadski/Zotero/storage/CQEBU39K/Gadzinski and Mocanu - 2025 - On Bitrates of Very Sparse Superposition Codes.pdf:application/pdf},
}

@misc{bachman_learning_2019,
	title = {Learning {Representations} by {Maximizing} {Mutual} {Information} {Across} {Views}},
	url = {https://arxiv.org/abs/1906.00910v2},
	abstract = {We propose an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple views of a shared context. For example, one could produce multiple views of a local spatio-temporal context by observing it from different locations (e.g., camera positions within a scene), and via different modalities (e.g., tactile, auditory, or visual). Or, an ImageNet image could provide a context from which one produces multiple views by repeatedly applying data augmentation. Maximizing mutual information between features extracted from these views requires capturing information about high-level factors whose influence spans multiple views -- e.g., presence of certain objects or occurrence of certain events. Following our proposed approach, we develop a model which learns image representations that significantly outperform prior methods on the tasks we consider. Most notably, using self-supervised learning, our model learns representations which achieve 68.1\% accuracy on ImageNet using standard linear evaluation. This beats prior results by over 12\% and concurrent results by 7\%. When we extend our model to use mixture-based representations, segmentation behaviour emerges as a natural side-effect. Our code is available online: https://github.com/Philip-Bachman/amdim-public.},
	language = {en},
	urldate = {2025-03-19},
	journal = {arXiv.org},
	author = {Bachman, Philip and Hjelm, R. Devon and Buchwalter, William},
	month = jun,
	year = {2019},
	file = {Full Text PDF:/Users/cgadski/Zotero/storage/M4J5GDQ8/Bachman et al. - 2019 - Learning Representations by Maximizing Mutual Info.pdf:application/pdf},
}

@misc{sharkey_open_2025,
	title = {Open {Problems} in {Mechanistic} {Interpretability}},
	url = {http://arxiv.org/abs/2501.16496},
	doi = {10.48550/arXiv.2501.16496},
	abstract = {Mechanistic interpretability aims to understand the computational mechanisms underlying neural networks' capabilities in order to accomplish concrete scientific and engineering goals. Progress in this field thus promises to provide greater assurance over AI system behavior and shed light on exciting scientific questions about the nature of intelligence. Despite recent progress toward these goals, there are many open problems in the field that require solutions before many scientific and practical benefits can be realized: Our methods require both conceptual and practical improvements to reveal deeper insights; we must figure out how best to apply our methods in pursuit of specific goals; and the field must grapple with socio-technical challenges that influence and are influenced by our work. This forward-facing review discusses the current frontier of mechanistic interpretability and the open problems that the field may benefit from prioritizing.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Sharkey, Lee and Chughtai, Bilal and Batson, Joshua and Lindsey, Jack and Wu, Jeff and Bushnaq, Lucius and Goldowsky-Dill, Nicholas and Heimersheim, Stefan and Ortega, Alejandro and Bloom, Joseph and Biderman, Stella and Garriga-Alonso, Adria and Conmy, Arthur and Nanda, Neel and Rumbelow, Jessica and Wattenberg, Martin and Schoots, Nandi and Miller, Joseph and Michaud, Eric J. and Casper, Stephen and Tegmark, Max and Saunders, William and Bau, David and Todd, Eric and Geiger, Atticus and Geva, Mor and Hoogland, Jesse and Murfet, Daniel and McGrath, Tom},
	month = jan,
	year = {2025},
	note = {arXiv:2501.16496 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/CN5PFAQ6/Sharkey et al. - 2025 - Open Problems in Mechanistic Interpretability.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/WSVMBPDH/2501.html:text/html},
}

@inproceedings{bergeaud_matching_1995,
	title = {Matching pursuit of images},
	volume = {1},
	url = {https://ieeexplore.ieee.org/abstract/document/529037},
	doi = {10.1109/ICIP.1995.529037},
	abstract = {A crucial problem in image analysis is to construct efficient low-level representations of an image, providing precise characterization of features which compose it, such as edges and texture components. An image usually contains very different types of features, which have been successfully modelled by the very redundant family of 2D Gabor oriented wavelets, describing the local properties of the image: localization, scale, preferred orientation, amplitude and phase of the discontinuity. However, this model generates representations of very large size. Instead of decomposing a given image over this whole set of Gabor functions, we use an adaptive algorithm (called matching pursuit) to select the Gabor elements which approximate at best the image, corresponding to the main features of the image. This produces compact representation in terms of few features that reveal the local image properties. Results proved that the elements are precisely localized on the edges of the images, and give a local decomposition as linear combinations of "textons" in the textured regions. We introduce a fast algorithm to compute the matching pursuit decomposition.},
	urldate = {2025-01-28},
	booktitle = {Proceedings., {International} {Conference} on {Image} {Processing}},
	author = {Bergeaud, F. and Mallat, S.},
	month = oct,
	year = {1995},
	keywords = {Mathematics, Matching pursuit algorithms, Data compression, Dictionaries, Explosions, Image analysis, Image edge detection, Image representation, Image texture analysis, Pursuit algorithms},
	pages = {53--56 vol.1},
	file = {IEEE Xplore Abstract Record:/Users/cgadski/Zotero/storage/YYG5PMYE/529037.html:text/html},
}

@article{joseph_fast_2014,
	title = {Fast {Sparse} {Superposition} {Codes} {Have} {Near} {Exponential} {Error} {Probability} for \${R}{\textless}\{{\textbackslash}cal {C}\}\$},
	volume = {60},
	issn = {0018-9448},
	url = {https://doi.org/10.1109/TIT.2013.2289865},
	doi = {10.1109/TIT.2013.2289865},
	abstract = {For the additive white Gaussian noise channel with average codeword power constraint, sparse superposition codes are developed. These codes are based on the statistical high-dimensional regression framework. In a previous paper, we investigated decoding using the optimal maximum-likelihood decoding scheme. Here, a fast decoding algorithm, called the adaptive successive decoder, is developed. For any rate  \$R\$ less than the capacity \$\{{\textbackslash}cal C\}\$, communication is shown to be reliable with nearly exponentially small error probability. Specifically, for blocklength \$n\$, it is shown that the error probability is exponentially small in \$n/{\textbackslash}log n\$.},
	number = {2},
	urldate = {2025-01-28},
	journal = {IEEE Trans. Inf. Theor.},
	author = {Joseph, Antony and Barron, Andrew R.},
	month = feb,
	year = {2014},
	pages = {919--942},
}

@misc{engels_not_2024,
	title = {Not {All} {Language} {Model} {Features} {Are} {Linear}},
	url = {http://arxiv.org/abs/2405.14860},
	doi = {10.48550/arXiv.2405.14860},
	abstract = {Recent work has proposed that language models perform computation by manipulating one-dimensional representations of concepts ("features") in activation space. In contrast, we explore whether some language model representations may be inherently multi-dimensional. We begin by developing a rigorous definition of irreducible multi-dimensional features based on whether they can be decomposed into either independent or non-co-occurring lower-dimensional features. Motivated by these definitions, we design a scalable method that uses sparse autoencoders to automatically find multi-dimensional features in GPT-2 and Mistral 7B. These auto-discovered features include strikingly interpretable examples, e.g. circular features representing days of the week and months of the year. We identify tasks where these exact circles are used to solve computational problems involving modular arithmetic in days of the week and months of the year. Next, we provide evidence that these circular features are indeed the fundamental unit of computation in these tasks with intervention experiments on Mistral 7B and Llama 3 8B. Finally, we find further circular representations by breaking down the hidden states for these tasks into interpretable components, and we examine the continuity of the days of the week feature in Mistral 7B.},
	urldate = {2025-01-26},
	publisher = {arXiv},
	author = {Engels, Joshua and Michaud, Eric J. and Liao, Isaac and Gurnee, Wes and Tegmark, Max},
	month = oct,
	year = {2024},
	note = {arXiv:2405.14860 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/CMSNQHE4/Engels et al. - 2024 - Not All Language Model Features Are Linear.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/UIKHKZ8Q/2405.html:text/html},
}

@book{vershynin_high-dimensional_2018,
	address = {Cambridge},
	series = {Cambridge {Series} in {Statistical} and {Probabilistic} {Mathematics}},
	title = {High-{Dimensional} {Probability}: {An} {Introduction} with {Applications} in {Data} {Science}},
	isbn = {978-1-108-41519-4},
	shorttitle = {High-{Dimensional} {Probability}},
	url = {https://www.cambridge.org/core/books/highdimensional-probability/797C466DA29743D2C8213493BD2D2102},
	abstract = {High-dimensional probability offers insight into the behavior of random vectors, random matrices, random subspaces, and objects used to quantify uncertainty in high dimensions. Drawing on ideas from probability, analysis, and geometry, it lends itself to applications in mathematics, statistics, theoretical computer science, signal processing, optimization, and more. It is the first to integrate theory, key tools, and modern applications of high-dimensional probability. Concentration inequalities form the core, and it covers both classical results such as Hoeffding's and Chernoff's inequalities and modern developments such as the matrix Bernstein's inequality. It then introduces the powerful methods based on stochastic processes, including such tools as Slepian's, Sudakov's, and Dudley's inequalities, as well as generic chaining and bounds based on VC dimension. A broad range of illustrations is embedded throughout, including classical and modern results for covariance estimation, clustering, networks, semidefinite programming, coding, dimension reduction, matrix completion, machine learning, compressed sensing, and sparse regression.},
	urldate = {2025-01-24},
	publisher = {Cambridge University Press},
	author = {Vershynin, Roman},
	year = {2018},
	doi = {10.1017/9781108231596},
	file = {Snapshot:/Users/cgadski/Zotero/storage/RN5ZLX4N/797C466DA29743D2C8213493BD2D2102.html:text/html;Submitted Version:/Users/cgadski/Zotero/storage/BTSPG5AM/Vershynin - 2018 - High-Dimensional Probability An Introduction with.pdf:application/pdf},
}

@book{thomas_elements_2006,
	title = {Elements of information theory},
	publisher = {Wiley-Interscience},
	author = {Thomas, MTCAJ and Joy, A. Thomas},
	year = {2006},
}

@inproceedings{tishby_deep_2015,
	title = {Deep learning and the information bottleneck principle},
	url = {https://ieeexplore.ieee.org/abstract/document/7133169},
	doi = {10.1109/ITW.2015.7133169},
	abstract = {Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
	urldate = {2025-01-23},
	booktitle = {2015 {IEEE} {Information} {Theory} {Workshop} ({ITW})},
	author = {Tishby, Naftali and Zaslavsky, Noga},
	month = apr,
	year = {2015},
	keywords = {Mutual information, Bifurcation, Complexity theory, Computer architecture, Distortion, Feature extraction, Training},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:/Users/cgadski/Zotero/storage/R5ZPW7K9/7133169.html:text/html;Submitted Version:/Users/cgadski/Zotero/storage/UIBX96LT/Tishby and Zaslavsky - 2015 - Deep learning and the information bottleneck princ.pdf:application/pdf},
}

@article{meng_locating_2022,
	title = {Locating and editing factual associations in {GPT}},
	volume = {35},
	urldate = {2025-01-23},
	journal = {Advances in Neural Information Processing Systems},
	author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
	year = {2022},
	pages = {17359--17372},
	file = {Available Version (via Google Scholar):/Users/cgadski/Zotero/storage/9TZVNZAL/Meng et al. - 2022 - Locating and editing factual associations in GPT.pdf:application/pdf},
}

@inproceedings{faruqui_sparse_2015,
	address = {Beijing, China},
	title = {Sparse {Overcomplete} {Word} {Vector} {Representations}},
	url = {https://aclanthology.org/P15-1144/},
	doi = {10.3115/v1/P15-1144},
	urldate = {2025-01-20},
	booktitle = {Proceedings of the 53rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 7th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Faruqui, Manaal and Tsvetkov, Yulia and Yogatama, Dani and Dyer, Chris and Smith, Noah A.},
	editor = {Zong, Chengqing and Strube, Michael},
	month = jul,
	year = {2015},
	pages = {1491--1500},
	file = {Full Text PDF:/Users/cgadski/Zotero/storage/UF4W8MZJ/Faruqui et al. - 2015 - Sparse Overcomplete Word Vector Representations.pdf:application/pdf},
}

@inproceedings{yun_transformer_2021,
	address = {Online},
	title = {Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors},
	shorttitle = {Transformer visualization via dictionary learning},
	url = {https://aclanthology.org/2021.deelio-1.1/},
	doi = {10.18653/v1/2021.deelio-1.1},
	abstract = {Transformer networks have revolutionized NLP representation learning since they were introduced. Though a great effort has been made to explain the representation in transformers, it is widely recognized that our understanding is not sufficient. One important reason is that there lack enough visualization tools for detailed analysis. In this paper, we propose to use dictionary learning to open up these ‘black boxes' as linear superpositions of transformer factors. Through visualization, we demonstrate the hierarchical semantic structures captured by the transformer factors, e.g., word-level polysemy disambiguation, sentence-level pattern formation, and long-range dependency. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights. We hope this visualization tool can bring further knowledge and a better understanding of how transformer networks work. The code is available at: https://github.com/zeyuyun1/TransformerVis.},
	urldate = {2025-01-20},
	booktitle = {Proceedings of {Deep} {Learning} {Inside} {Out} ({DeeLIO}): {The} 2nd {Workshop} on {Knowledge} {Extraction} and {Integration} for {Deep} {Learning} {Architectures}},
	publisher = {Association for Computational Linguistics},
	author = {Yun, Zeyu and Chen, Yubei and Olshausen, Bruno and LeCun, Yann},
	editor = {Agirre, Eneko and Apidianaki, Marianna and Vulić, Ivan},
	month = jun,
	year = {2021},
	pages = {1--10},
	file = {Full Text PDF:/Users/cgadski/Zotero/storage/HBCDBD87/Yun et al. - 2021 - Transformer visualization via dictionary learning.pdf:application/pdf},
}

@inproceedings{zhang_sample_2023,
	title = {A sample survey study of poly-semantic neurons in deep {CNNs}},
	volume = {12604},
	doi = {10.1117/12.2674650},
	abstract = {Although deep CNN networks have excellent image classification performance, they do not provide interpretability, and furthermore existing work reveals that these models have complex internals, for example, mysterious polysemantic neurons activate to multiple features. In this work, we analyze the intermediate data of the network dissection paper made by Bau et al. to understand to what extent polysemantic neurons exist. We divide the polysemantic neurons into five types and calculate the percentage of each type by sampling. We find that above 50\% neurons identify one concept but there are a quite proportion of neurons that recognize two or more features. This can explain the high classification accuracy and some capacity saving of a deep CNN. By case studies, we draw some conclusions and hypotheses: First, unlike the human visual system, a CNN cannot distinguish detailed features (metaphor: a CNN is like a nearsighted eye). Second, the reason that the CNN is prone to adversarial attacks may be partially due to the polysemantic neurons. Third, polysemantic neurons may partially explain why people wrongly visualize one thing as another in neuroscience.},
	urldate = {2025-01-17},
	booktitle = {International {Conference} on {Computer} {Graphics}, {Artificial} {Intelligence}, and {Data} {Processing} ({ICCAID} 2022)},
	publisher = {SPIE},
	author = {Zhang, Changwan and Wang, Yue},
	month = may,
	year = {2023},
	pages = {849--855},
}

@book{rumelhart_parallel_1986,
	title = {Parallel {Distributed} {Processing}: {Explorations} in the {Microstructure} of {Cognition}: {Foundations}},
	isbn = {978-0-262-29140-8},
	shorttitle = {Parallel {Distributed} {Processing}},
	language = {en},
	urldate = {2024-10-22},
	publisher = {The MIT Press},
	author = {Rumelhart, David E. and McClelland, James L. and {AU}},
	year = {1986},
	doi = {10.7551/mitpress/5236.001.0001},
}

@article{templeton_scaling_2024,
	title = {Scaling {Monosemanticity}: {Extracting} {Interpretable} {Features} from {Claude} 3 {Sonnet}},
	url = {https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html},
	urldate = {2024-07-23},
	journal = {Transformer Circuits Thread},
	author = {Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
	month = may,
	year = {2024},
	file = {Scaling Monosemanticity\: Extracting Interpretable Features from Claude 3 Sonnet:/Users/cgadski/Zotero/storage/WTGXGKHU/index.html:text/html},
}

@misc{frady_theory_2017,
	title = {Theory of the superposition principle for randomized connectionist representations in neural networks},
	url = {http://arxiv.org/abs/1707.01429},
	doi = {10.48550/arXiv.1707.01429},
	abstract = {To understand cognitive reasoning in the brain, it has been proposed that symbols and compositions of symbols are represented by activity patterns (vectors) in a large population of neurons. Formal models implementing this idea [Plate 2003], [Kanerva 2009], [Gayler 2003], [Eliasmith 2012] include a reversible superposition operation for representing with a single vector an entire set of symbols or an ordered sequence of symbols. If the representation space is high-dimensional, large sets of symbols can be superposed and individually retrieved. However, crosstalk noise limits the accuracy of retrieval and information capacity. To understand information processing in the brain and to design artificial neural systems for cognitive reasoning, a theory of this superposition operation is essential. Here, such a theory is presented. The superposition operations in different existing models are mapped to linear neural networks with unitary recurrent matrices, in which retrieval accuracy can be analyzed by a single equation. We show that networks representing information in superposition can achieve a channel capacity of about half a bit per neuron, a significant fraction of the total available entropy. Going beyond existing models, superposition operations with recency effects are proposed that avoid catastrophic forgetting when representing the history of infinite data streams. These novel models correspond to recurrent networks with non-unitary matrices or with nonlinear neurons, and can be analyzed and optimized with an extension of our theory.},
	urldate = {2025-01-20},
	publisher = {arXiv},
	author = {Frady, E. Paxon and Kleyko, Denis and Sommer, Friedrich T.},
	month = jul,
	year = {2017},
	note = {arXiv:1707.01429 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/YILP8UPC/Frady et al. - 2017 - Theory of the superposition principle for randomiz.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/NMVKUT4E/1707.html:text/html},
}

@phdthesis{zadik_computational_2019,
	type = {Thesis},
	title = {Computational and statistical challenges in high dimensional statistical models},
	copyright = {MIT theses are protected by copyright. They may be viewed, downloaded, or printed from this source but further reproduction or distribution in any format is prohibited without written permission.},
	url = {https://dspace.mit.edu/handle/1721.1/123708},
	abstract = {This thesis focuses on two long-studied high-dimensional statistical models, namely (1) the high-dimensional linear regression (HDLR) model, where the goal is to recover a hidden vector of coefficients from noisy linear observations, and (2) the planted clique (PC) model, where the goal is to recover a hidden community structure from a much larger observed network. The following results are established. First, under assumptions, we identify the exact statistical limit of the model, that is the minimum signal strength allowing a statistically accurate inference of the hidden vector. We couple this result with an all-or-nothing information theoretic (IT) phase transition. We prove that above the statistical limit, it is IT possible to almost-perfectly recover the hidden vector, while below the statistical limit, it is IT impossible to achieve non-trivial correlation with the hidden vector.},
	language = {eng},
	urldate = {2025-01-20},
	school = {Massachusetts Institute of Technology},
	author = {Zadik, Ilias},
	year = {2019},
	note = {Accepted: 2020-02-10T21:37:19Z},
	file = {Full Text PDF:/Users/cgadski/Zotero/storage/VZ4EFVWH/Zadik - 2019 - Computational and statistical challenges in high d.pdf:application/pdf},
}

@inproceedings{reeves_all-or-nothing_2019,
	title = {The {All}-or-{Nothing} {Phenomenon} in {Sparse} {Linear} {Regression}},
	url = {https://proceedings.mlr.press/v99/reeves19a.html},
	abstract = {We study the problem of recovering a hidden binary kkk-sparse ppp-dimensional vector ββ{\textbackslash}beta from nnn noisy linear observations Y=Xβ+WY=Xβ+WY=X{\textbackslash}beta+W where XijXijX\_\{ij\} are i.i.d.  N(0,1)N(0,1){\textbackslash}mathcal\{N\}(0,1) and WiWiW\_i are i.i.d.  N(0,σ2)N(0,σ2){\textbackslash}mathcal\{N\}(0,{\textbackslash}sigma{\textasciicircum}2). A closely related  hypothesis testing problem is to distinguish the pair (X,Y)(X,Y)(X,Y) generated from this structured model from a corresponding null model where (X,Y)(X,Y)(X,Y) consist of purely independent Gaussian entries. In the low sparsity k=o(p)k=o(p)k=o(p) and high signal to noise ratio k/σ2=Ω(1)k/σ2=Ω(1)k/{\textbackslash}sigma{\textasciicircum}2={\textbackslash}Omega{\textbackslash}left(1{\textbackslash}right) regime, we establish an “All-or-Nothing” information-theoretic phase transition at a critical sample size n∗=2klog(p/k)/log(1+k/σ2)n∗=2klog⁡(p/k)/log⁡(1+k/σ2)n{\textasciicircum}*=2 k{\textbackslash}log {\textbackslash}left(p/k{\textbackslash}right) /{\textbackslash}log {\textbackslash}left(1+k/{\textbackslash}sigma{\textasciicircum}2{\textbackslash}right), resolving a conjecture of [GamarnikZadik17]. Specifically, we show that if lim infp→∞n/n∗{\textgreater}1lim infp→∞n/n∗{\textgreater}1{\textbackslash}liminf\_\{p{\textbackslash}rightarrow {\textbackslash}infty\} n/n{\textasciicircum}*{\textgreater}1, then the maximum likelihood estimator almost perfectly recovers the hidden vector with high probability and moreover the true hypothesis can be detected with a vanishing error probability. Conversely, if lim supp→∞n/n∗{\textless}1lim supp→∞n/n∗{\textless}1{\textbackslash}limsup\_\{p{\textbackslash}rightarrow {\textbackslash}infty\} n/n{\textasciicircum}*{\textless}1, then it becomes information-theoretically impossible even to  recover an arbitrarily small but fixed fraction of the hidden vector support, or to test hypotheses strictly better than random guess. Our proof of the impossibility result builds upon two key techniques, which could be of independent interest. First, we use a conditional second moment method to upper bound the Kullback-Leibler (KL) divergence between the structured and the null model. Second, inspired by the celebrated area theorem, we establish a lower bound to the minimum mean squared estimation error of the hidden vector in terms of the KL divergence between the two models.},
	language = {en},
	urldate = {2025-01-19},
	booktitle = {Proceedings of the {Thirty}-{Second} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Reeves, Galen and Xu, Jiaming and Zadik, Ilias},
	month = jun,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2652--2663},
	file = {Full Text PDF:/Users/cgadski/Zotero/storage/ATHMRPQF/Reeves et al. - 2019 - The All-or-Nothing Phenomenon in Sparse Linear Reg.pdf:application/pdf},
}

@misc{noauthor_distributed_nodate,
	title = {Distributed {Representations}: {Composition} \& {Superposition}},
	url = {https://transformer-circuits.pub/2023/superposition-composition/index.html},
	urldate = {2025-01-19},
	file = {Distributed Representations\: Composition & Superposition:/Users/cgadski/Zotero/storage/YYULXIEY/index.html:text/html},
}

@article{olshausen_sparse_1997,
	title = {Sparse coding with an overcomplete basis set: {A} strategy employed by {V1}?},
	volume = {37},
	issn = {0042-6989},
	shorttitle = {Sparse coding with an overcomplete basis set},
	url = {https://www.sciencedirect.com/science/article/pii/S0042698997001697},
	doi = {10.1016/S0042-6989(97)00169-7},
	abstract = {The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete—i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.},
	number = {23},
	urldate = {2025-01-18},
	journal = {Vision Research},
	author = {Olshausen, Bruno A. and Field, David J.},
	month = dec,
	year = {1997},
	keywords = {Coding, Gabor-wavelet, Natural images, V1},
	pages = {3311--3325},
	file = {ScienceDirect Snapshot:/Users/cgadski/Zotero/storage/MPQ2ZMDU/S0042698997001697.html:text/html},
}

@misc{arora_simple_2015,
	title = {Simple, {Efficient}, and {Neural} {Algorithms} for {Sparse} {Coding}},
	url = {http://arxiv.org/abs/1503.00778},
	doi = {10.48550/arXiv.1503.00778},
	abstract = {Sparse coding is a basic task in many fields including signal processing, neuroscience and machine learning where the goal is to learn a basis that enables a sparse representation of a given set of data, if one exists. Its standard formulation is as a non-convex optimization problem which is solved in practice by heuristics based on alternating minimization. Re- cent work has resulted in several algorithms for sparse coding with provable guarantees, but somewhat surprisingly these are outperformed by the simple alternating minimization heuristics. Here we give a general framework for understanding alternating minimization which we leverage to analyze existing heuristics and to design new ones also with provable guarantees. Some of these algorithms seem implementable on simple neural architectures, which was the original motivation of Olshausen and Field (1997a) in introducing sparse coding. We also give the first efficient algorithm for sparse coding that works almost up to the information theoretic limit for sparse recovery on incoherent dictionaries. All previous algorithms that approached or surpassed this limit run in time exponential in some natural parameter. Finally, our algorithms improve upon the sample complexity of existing approaches. We believe that our analysis framework will have applications in other settings where simple iterative algorithms are used.},
	urldate = {2025-01-18},
	publisher = {arXiv},
	author = {Arora, Sanjeev and Ge, Rong and Ma, Tengyu and Moitra, Ankur},
	month = mar,
	year = {2015},
	note = {arXiv:1503.00778 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Data Structures and Algorithms},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/I92X65LJ/Arora et al. - 2015 - Simple, Efficient, and Neural Algorithms for Spars.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/U4PDY8DB/1503.html:text/html},
}

@misc{nguyen_multifaceted_2016,
	title = {Multifaceted {Feature} {Visualization}: {Uncovering} the {Different} {Types} of {Features} {Learned} {By} {Each} {Neuron} in {Deep} {Neural} {Networks}},
	shorttitle = {Multifaceted {Feature} {Visualization}},
	url = {http://arxiv.org/abs/1602.03616},
	doi = {10.48550/arXiv.1602.03616},
	abstract = {We can better understand deep neural networks by identifying which features each of their neurons have learned to detect. To do so, researchers have created Deep Visualization techniques including activation maximization, which synthetically generates inputs (e.g. images) that maximally activate each neuron. A limitation of current techniques is that they assume each neuron detects only one type of feature, but we know that neurons can be multifaceted, in that they fire in response to many different types of features: for example, a grocery store class neuron must activate either for rows of produce or for a storefront. Previous activation maximization techniques constructed images without regard for the multiple different facets of a neuron, creating inappropriate mixes of colors, parts of objects, scales, orientations, etc. Here, we introduce an algorithm that explicitly uncovers the multiple facets of each neuron by producing a synthetic visualization of each of the types of images that activate a neuron. We also introduce regularization methods that produce state-of-the-art results in terms of the interpretability of images obtained by activation maximization. By separately synthesizing each type of image a neuron fires in response to, the visualizations have more appropriate colors and coherent global structure. Multifaceted feature visualization thus provides a clearer and more comprehensive description of the role of each neuron.},
	urldate = {2025-01-18},
	publisher = {arXiv},
	author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
	month = may,
	year = {2016},
	note = {arXiv:1602.03616 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/R73QI4EB/Nguyen et al. - 2016 - Multifaceted Feature Visualization Uncovering the.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/H4ULAK4Z/1602.html:text/html},
}

@article{cohn_sphere_2014,
	title = {Sphere packing bounds via spherical codes},
	volume = {163},
	issn = {0012-7094},
	url = {http://arxiv.org/abs/1212.5966},
	doi = {10.1215/00127094-2738857},
	abstract = {The sphere packing problem asks for the greatest density of a packing of congruent balls in Euclidean space. The current best upper bound in all sufficiently high dimensions is due to Kabatiansky and Levenshtein in 1978. We revisit their argument and improve their bound by a constant factor using a simple geometric argument, and we extend the argument to packings in hyperbolic space, for which it gives an exponential improvement over the previously known bounds. Additionally, we show that the Cohn-Elkies linear programming bound is always at least as strong as the Kabatiansky-Levenshtein bound; this result is analogous to Rodemich's theorem in coding theory. Finally, we develop hyperbolic linear programming bounds and prove the analogue of Rodemich's theorem there as well.},
	number = {10},
	urldate = {2025-01-18},
	journal = {Duke Mathematical Journal},
	author = {Cohn, Henry and Zhao, Yufei},
	month = jul,
	year = {2014},
	note = {arXiv:1212.5966 [math]},
	keywords = {Mathematics - Metric Geometry},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/S9LD4MHL/Cohn and Zhao - 2014 - Sphere packing bounds via spherical codes.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/MPF96T92/1212.html:text/html},
}

@misc{barron_least_2010,
	title = {Least {Squares} {Superposition} {Codes} of {Moderate} {Dictionary} {Size}, {Reliable} at {Rates} up to {Capacity}},
	url = {http://arxiv.org/abs/1006.3780},
	doi = {10.48550/arXiv.1006.3780},
	abstract = {For the additive white Gaussian noise channel with average codeword power constraint, new coding methods are devised in which the codewords are sparse superpositions, that is, linear combinations of subsets of vectors from a given design, with the possible messages indexed by the choice of subset. Decoding is by least squares, tailored to the assumed form of linear combination. Communication is shown to be reliable with error probability exponentially small for all rates up to the Shannon capacity.},
	urldate = {2025-01-16},
	publisher = {arXiv},
	author = {Barron, Andrew R. and Joseph, Antony},
	month = jun,
	year = {2010},
	note = {arXiv:1006.3780 [cs]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Computer Science - Information Theory, Mathematics - Information Theory, Statistics - Statistics Theory},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/IJPBPV47/Barron and Joseph - 2010 - Least Squares Superposition Codes of Moderate Dict.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/QU4H8IL7/1006.html:text/html},
}

@article{joseph_least_2012,
	title = {Least {Squares} {Superposition} {Codes} of {Moderate} {Dictionary} {Size} {Are} {Reliable} at {Rates} up to {Capacity}},
	volume = {58},
	issn = {1557-9654},
	url = {https://ieeexplore.ieee.org/document/6142076},
	doi = {10.1109/TIT.2012.2184847},
	abstract = {For the additive white Gaussian noise channel with average codeword power constraint, coding methods are analyzed in which the codewords are sparse superpositions, that is, linear combinations of subsets of vectors from a given design, with the possible messages indexed by the choice of subset. Decoding is by least squares (maximum likelihood), tailored to the assumed form of codewords being linear combinations of elements of the design. Communication is shown to be reliable with error probability exponentially small for all rates up to the Shannon capacity.},
	number = {5},
	urldate = {2025-01-16},
	journal = {IEEE Transactions on Information Theory},
	author = {Joseph, Antony and Barron, Andrew R},
	month = may,
	year = {2012},
	note = {Conference Name: IEEE Transactions on Information Theory},
	keywords = {compressed sensing, Dictionaries, Achieving capacity, Capacity planning, Encoding, Error probability, exponential error bounds, Gaussian channel, Maximum likelihood decoding, maximum likelihood estimation, Reliability, subset selection},
	pages = {2541--2557},
}

@article{rush_capacity-achieving_2017,
	title = {Capacity-achieving {Sparse} {Superposition} {Codes} via {Approximate} {Message} {Passing} {Decoding}},
	issn = {0018-9448},
	url = {https://www.repository.cam.ac.uk/handle/1810/262895},
	doi = {10.17863/CAM.8183},
	abstract = {Sparse superposition codes were recently introduced by Barron and Joseph for reliable communication over the additive white Gaussian noise (AWGN) channel at rates approaching the channel capacity. The codebook is defined in terms of a Gaussian design matrix, and codewords are sparse linear combinations of columns of the matrix. In this paper, we propose an approximate message passing decoder for sparse superposition codes, whose decoding complexity scales linearly with the size of the design matrix. The performance of the decoder is rigorously analyzed and it is shown to asymptotically achieve the AWGN capacity with an appropriate power allocation. Simulation results are provided to demonstrate the performance of the decoder at finite blocklengths. We introduce a power allocation scheme to improve the empirical performance, and demonstrate how the decoding complexity can be significantly reduced by using Hadamard design matrices.},
	language = {eng},
	urldate = {2025-01-16},
	author = {Rush, C. and Greig, A. and Venkataramanan, R.},
	month = jan,
	year = {2017},
	note = {Publisher: IEEE},
	file = {Full Text PDF:/Users/cgadski/Zotero/storage/E3V7BBQ6/Rush et al. - 2017 - Capacity-achieving Sparse Superposition Codes via .pdf:application/pdf},
}

@book{wright_high-dimensional_2022,
	address = {Cambridge},
	edition = {New edition},
	title = {High-{Dimensional} {Data} {Analysis} with {Low}-{Dimensional} {Models}: {Principles}, {Computation}, and {Applications}},
	isbn = {978-1-108-48973-7},
	shorttitle = {High-{Dimensional} {Data} {Analysis} with {Low}-{Dimensional} {Models}},
	abstract = {Connecting theory with practice, this systematic and rigorous introduction covers the fundamental principles, algorithms and applications of key mathematical models for high-dimensional data analysis. Comprehensive in its approach, it provides unified coverage of many different low-dimensional models and analytical techniques, including sparse and low-rank models, and both convex and non-convex formulations. Readers will learn how to develop efficient and scalable algorithms for solving real-world problems, supported by numerous examples and exercises throughout, and how to use the computational tools learnt in several application contexts. Applications presented include scientific imaging, communication, face recognition, 3D vision, and deep networks for classification. With code available online, this is an ideal textbook for senior and graduate students in computer science, data science, and electrical engineering, as well as for those taking courses on sparsity, low-dimensional structures, and high-dimensional data. Foreword by Emmanuel Candès.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Wright, John and Ma, Yi},
	month = apr,
	year = {2022},
}

@misc{wright_high-dimensional_2022-1,
	title = {High-{Dimensional} {Data} {Analysis} with {Low}-{Dimensional} {Models}: {Principles}, {Computation}, and {Applications}},
	shorttitle = {High-{Dimensional} {Data} {Analysis} with {Low}-{Dimensional} {Models}},
	url = {https://www.cambridge.org/highereducation/books/high-dimensional-data-analysis-with-low-dimensional-models/05E9DE014DCE9E08D4AA014975D94B60},
	abstract = {Connecting theory with practice, this systematic and rigorous introduction covers the fundamental principles, algorithms and applications of key mathematical models for high-dimensional data analysis. Comprehensive in its approach, it provides unified coverage of many different low-dimensional models and analytical techniques, including sparse and low-rank models, and both convex and non-convex formulations. Readers will learn how to develop efficient and scalable algorithms for solving real-world problems, supported by numerous examples and exercises throughout, and how to use the computational tools learnt in several application contexts. Applications presented include scientific imaging, communication, face recognition, 3D vision, and deep networks for classification. With code available online, this is an ideal textbook for senior and graduate students in computer science, data science, and electrical engineering, as well as for those taking courses on sparsity, low-dimensional structures, and high-dimensional data. Foreword by Emmanuel Candès.},
	language = {en},
	urldate = {2025-01-13},
	journal = {Higher Education from Cambridge University Press},
	author = {Wright, John and Ma, Yi},
	month = jan,
	year = {2022},
	doi = {10.1017/9781108779302},
	note = {ISBN: 9781108779302
Publisher: Cambridge University Press},
}

@book{parikh_proximal_2013,
	title = {Proximal {Algorithms}},
	isbn = {978-1-60198-716-7},
	abstract = {Proximal Algorithms discusses proximal operators and proximal algorithms, and illustrates their applicability to standard and distributed convex optimization in general and many applications of recent interest in particular. Much like Newton's method is a standard tool for solving unconstrained smooth optimization problems of modest size, proximal algorithms can be viewed as an analogous tool for nonsmooth, constrained, large-scale, or distributed versions of these problems. They are very generally applicable, but are especially well-suited to problems of substantial recent interest involving large or high-dimensional datasets. Proximal methods sit at a higher level of abstraction than classical algorithms like Newton's method: the base operation is evaluating the proximal operator of a function, which itself involves solving a small convex optimization problem. These subproblems, which generalize the problem of projecting a point onto a convex set, often admit closed-form solutions or can be solved very quickly with standard or simple specialized methods. Proximal Algorithms discusses different interpretations of proximal operators and algorithms, looks at their connections to many other topics in optimization and applied mathematics, surveys some popular algorithms, and provides a large number of examples of proximal operators that commonly arise in practice.},
	language = {en},
	publisher = {Now Publishers},
	author = {Parikh, Neal and Boyd, Stephen},
	month = nov,
	year = {2013},
	note = {Google-Books-ID: DS04ngEACAAJ},
	keywords = {Mathematics / Optimization, Technology \& Engineering / Electrical},
}

@book{foucart_mathematical_2013,
	address = {New York},
	edition = {2013th edition},
	title = {A {Mathematical} {Introduction} to {Compressive} {Sensing}},
	isbn = {978-0-8176-4947-0},
	abstract = {At the intersection of mathematics, engineering, and computer science sits the thriving field of compressive sensing. Based on the premise that data acquisition and compression can be performed simultaneously, compressive sensing finds applications in imaging, signal processing, and many other domains. In the areas of applied mathematics, electrical engineering, and theoretical computer science, an explosion of research activity has already followed the theoretical results that highlighted the efficiency of the basic principles. The elegant ideas behind these principles are also of independent interest to pure mathematicians.A Mathematical Introduction to Compressive Sensing gives a detailed account of the core theory upon which the field is build. With only moderate prerequisites, it is an excellent textbook for graduate courses in mathematics, engineering, and computer science. It also serves as a reliable resource for practitioners and researchers in these disciplines who want to acquire a careful understanding of the subject. A Mathematical Introduction to Compressive Sensing uses a mathematical perspective to present the core of the theory underlying compressive sensing.},
	language = {English},
	publisher = {Birkhäuser},
	author = {Foucart, Simon and Rauhut, Holger},
	month = aug,
	year = {2013},
}

@misc{arora_implicit_2019,
	title = {Implicit {Regularization} in {Deep} {Matrix} {Factorization}},
	url = {http://arxiv.org/abs/1905.13655},
	doi = {10.48550/arXiv.1905.13655},
	abstract = {Efforts to understand the generalization mystery in deep learning have led to the belief that gradient-based optimization induces a form of implicit regularization, a bias towards models of low "complexity." We study the implicit regularization of gradient descent over deep linear neural networks for matrix completion and sensing, a model referred to as deep matrix factorization. Our first finding, supported by theory and experiments, is that adding depth to a matrix factorization enhances an implicit tendency towards low-rank solutions, oftentimes leading to more accurate recovery. Secondly, we present theoretical and empirical arguments questioning a nascent view by which implicit regularization in matrix factorization can be captured using simple mathematical norms. Our results point to the possibility that the language of standard regularizers may not be rich enough to fully encompass the implicit regularization brought forth by gradient-based optimization.},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
	month = oct,
	year = {2019},
	note = {arXiv:1905.13655 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/JTHQXX3K/Arora et al. - 2019 - Implicit Regularization in Deep Matrix Factorizati.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/BGLA2Y7A/1905.html:text/html},
}

@misc{adlam_kernel_2023,
	title = {Kernel {Regression} with {Infinite}-{Width} {Neural} {Networks} on {Millions} of {Examples}},
	url = {http://arxiv.org/abs/2303.05420},
	doi = {10.48550/arXiv.2303.05420},
	abstract = {Neural kernels have drastically increased performance on diverse and nonstandard data modalities but require significantly more compute, which previously limited their application to smaller datasets. In this work, we address this by massively parallelizing their computation across many GPUs. We combine this with a distributed, preconditioned conjugate gradients algorithm to enable kernel regression at a large scale (i.e. up to five million examples). Using this approach, we study scaling laws of several neural kernels across many orders of magnitude for the CIFAR-5m dataset. Using data augmentation to expand the original CIFAR-10 training dataset by a factor of 20, we obtain a test accuracy of 91.2{\textbackslash}\% (SotA for a pure kernel method). Moreover, we explore neural kernels on other data modalities, obtaining results on protein and small molecule prediction tasks that are competitive with SotA methods.},
	urldate = {2024-12-18},
	publisher = {arXiv},
	author = {Adlam, Ben and Lee, Jaehoon and Padhy, Shreyas and Nado, Zachary and Snoek, Jasper},
	month = mar,
	year = {2023},
	note = {arXiv:2303.05420 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/DYAVPGUZ/Adlam et al. - 2023 - Kernel Regression with Infinite-Width Neural Netwo.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/S8GFBBVK/2303.html:text/html},
}

@inproceedings{mikolov_linguistic_2013,
	address = {Atlanta, Georgia},
	title = {Linguistic {Regularities} in {Continuous} {Space} {Word} {Representations}},
	url = {https://aclanthology.org/N13-1090},
	urldate = {2024-12-02},
	booktitle = {Proceedings of the 2013 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
	editor = {Vanderwende, Lucy and Daumé III, Hal and Kirchhoff, Katrin},
	month = jun,
	year = {2013},
	pages = {746--751},
	file = {Full Text PDF:/Users/cgadski/Zotero/storage/KPM3SIIF/Mikolov et al. - 2013 - Linguistic Regularities in Continuous Space Word R.pdf:application/pdf},
}

@misc{goh_decoding_2016,
	title = {Decoding the {Thought} {Vector}},
	url = {https://gabgoh.github.io/ThoughtVectors/},
	urldate = {2024-07-23},
	author = {Goh, Gabriel},
	year = {2016},
	file = {Decoding the Thought Vector:/Users/cgadski/Zotero/storage/58UMH6KD/ThoughtVectors.html:text/html},
}

@misc{park_linear_2024,
	title = {The {Linear} {Representation} {Hypothesis} and the {Geometry} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2311.03658},
	doi = {10.48550/arXiv.2311.03658},
	abstract = {Informally, the 'linear representation hypothesis' is the idea that high-level concepts are represented linearly as directions in some representation space. In this paper, we address two closely related questions: What does "linear representation" actually mean? And, how do we make sense of geometric notions (e.g., cosine similarity or projection) in the representation space? To answer these, we use the language of counterfactuals to give two formalizations of "linear representation", one in the output (word) representation space, and one in the input (sentence) space. We then prove these connect to linear probing and model steering, respectively. To make sense of geometric notions, we use the formalization to identify a particular (non-Euclidean) inner product that respects language structure in a sense we make precise. Using this causal inner product, we show how to unify all notions of linear representation. In particular, this allows the construction of probes and steering vectors using counterfactual pairs. Experiments with LLaMA-2 demonstrate the existence of linear representations of concepts, the connection to interpretation and control, and the fundamental role of the choice of inner product.},
	urldate = {2024-12-02},
	publisher = {arXiv},
	author = {Park, Kiho and Choe, Yo Joong and Veitch, Victor},
	month = jul,
	year = {2024},
	note = {arXiv:2311.03658},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/IXLKKGV7/Park et al. - 2024 - The Linear Representation Hypothesis and the Geome.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/WZ3XSLAY/2311.html:text/html},
}

@incollection{foucart_sparse_2013,
	address = {New York, NY},
	title = {Sparse {Recovery} with {Random} {Matrices}},
	isbn = {978-0-8176-4948-7},
	url = {https://doi.org/10.1007/978-0-8176-4948-7_9},
	abstract = {In this chapter, the restricted isometry property, which guarantees the uniform recovery of sparse vectors via a variety of methods, is proved to hold with high probability for subgaussian random matrices provided the number of rows (i.e., measurements) scales like the sparsity times a logarithmic factor. For Gaussian matrices, precise estimates for the required number of measurements (including optimal or at least small values of the constants) are given both in the setting of nonuniform recovery and of uniform recovery. In the latter case, this is first done via an estimate of the restricted isometry constants and then directly through the null space property. Finally, a close relation between the restricted isometry property and the Johnson–Lindenstrauss lemma is uncovered.},
	language = {en},
	urldate = {2024-11-30},
	booktitle = {A {Mathematical} {Introduction} to {Compressive} {Sensing}},
	publisher = {Springer},
	author = {Foucart, Simon and Rauhut, Holger},
	editor = {Foucart, Simon and Rauhut, Holger},
	year = {2013},
	doi = {10.1007/978-0-8176-4948-7_9},
	keywords = {concentration inequality, Gaussian width, Gordon’s escape through the mesh, isotropic subgaussian vectors, Johnson–Lindenstrauss lemma, ℓ
              1-minimization, nonuniform recovery, null space property, restricted isometry property, subgaussian matrices, uniform recovery, universality},
	pages = {271--310},
	file = {Full Text PDF:/Users/cgadski/Zotero/storage/ICNHAJVH/Foucart and Rauhut - 2013 - Sparse Recovery with Random Matrices.pdf:application/pdf},
}

@incollection{foucart_advanced_2013,
	address = {New York, NY},
	title = {Advanced {Tools} from {Probability} {Theory}},
	isbn = {978-0-8176-4948-7},
	url = {https://doi.org/10.1007/978-0-8176-4948-7_8},
	abstract = {This chapter is dedicated to advanced probability results required in the more involved arguments on random measurement matrices, notably precise bounds for Gaussian random matrices and the analysis of random partial Fourier matrices. First, norms of Gaussian vectors in expectation are discussed, followed by Rademacher sums and the symmetrization principle. Khintchine inequalities provide bounds for moments of Rademacher sums. Then decoupling inequalities are covered. They allow one to simplify the analysis of double sums of random variables by replacing one instance of a random vector by an independent copy. The noncommutative Bernstein inequality treated next bounds the tail of a sum of independent random matrices in the operator norm. Dudley’s inequality bounds the supremum of a subgaussian process by an integral over covering numbers with respect to the index set of the process. Slepian’s and Gordon’s lemma compare certain functions of Gaussian vectors in terms of their covariance structures. Together with the concentration of measure principle for Lipschitz functions of Gaussian vectors covered next, they provide powerful tools for the analysis of Gaussian random matrices. Finally, the chapter discusses Talagrand’s inequality, i.e., a Bernstein-type inequality for suprema of empirical processes.},
	language = {en},
	urldate = {2024-11-30},
	booktitle = {A {Mathematical} {Introduction} to {Compressive} {Sensing}},
	publisher = {Springer},
	author = {Foucart, Simon and Rauhut, Holger},
	editor = {Foucart, Simon and Rauhut, Holger},
	year = {2013},
	doi = {10.1007/978-0-8176-4948-7_8},
	keywords = {chaining, concentration of measure, decoupling, Dudley’s inequality, entropy, Gordon’s lemma, Khintchine inequalities, noncommutative Bernstein inequalities, norms of Gaussian vectors, Rademacher chaos, Rademacher sums, Slepian’s lemma, Steinhaus sums, stochastic process, suprema of empirical processes, symmetrization, Talagrand’s concentration inequality},
	pages = {201--269},
	file = {Full Text PDF:/Users/cgadski/Zotero/storage/BUXT2269/Foucart and Rauhut - 2013 - Advanced Tools from Probability Theory.pdf:application/pdf},
}

@misc{makhzani_k-sparse_2014,
	title = {k-{Sparse} {Autoencoders}},
	url = {http://arxiv.org/abs/1312.5663},
	doi = {10.48550/arXiv.1312.5663},
	abstract = {Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is an autoencoder with linear activation function, where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and RBMs. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.},
	urldate = {2024-11-29},
	publisher = {arXiv},
	author = {Makhzani, Alireza and Frey, Brendan},
	month = mar,
	year = {2014},
	note = {arXiv:1312.5663},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/TSNVNBFE/Makhzani and Frey - 2014 - k-Sparse Autoencoders.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/DGUWXMDA/1312.html:text/html},
}

@misc{rajamanoharan_improving_2024,
	title = {Improving {Dictionary} {Learning} with {Gated} {Sparse} {Autoencoders}},
	url = {http://arxiv.org/abs/2404.16014},
	doi = {10.48550/arXiv.2404.16014},
	abstract = {Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models' (LMs) activations, by finding sparse, linear reconstructions of LM activations. We introduce the Gated Sparse Autoencoder (Gated SAE), which achieves a Pareto improvement over training with prevailing methods. In SAEs, the L1 penalty used to encourage sparsity introduces many undesirable biases, such as shrinkage -- systematic underestimation of feature activations. The key insight of Gated SAEs is to separate the functionality of (a) determining which directions to use and (b) estimating the magnitudes of those directions: this enables us to apply the L1 penalty only to the former, limiting the scope of undesirable side effects. Through training SAEs on LMs of up to 7B parameters we find that, in typical hyper-parameter ranges, Gated SAEs solve shrinkage, are similarly interpretable, and require half as many firing features to achieve comparable reconstruction fidelity.},
	urldate = {2024-11-29},
	publisher = {arXiv},
	author = {Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Lieberum, Tom and Varma, Vikrant and Kramár, János and Shah, Rohin and Nanda, Neel},
	month = apr,
	year = {2024},
	note = {arXiv:2404.16014},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/KJC5LS56/Rajamanoharan et al. - 2024 - Improving Dictionary Learning with Gated Sparse Au.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/IGKA5NM6/2404.html:text/html},
}

@book{elad_sparse_2010,
	address = {New York},
	title = {Sparse and redundant representations: from theory to applications in signal and image processing},
	isbn = {978-1-4419-7010-7 978-1-4419-7011-4},
	shorttitle = {Sparse and redundant representations},
	publisher = {Springer},
	author = {Elad, M.},
	year = {2010},
	note = {OCLC: ocn646114450},
	keywords = {Mathematics, Functional analysis, Signal processing, Analyse fonctionnelle, Image processing, Mathématiques, Traitement d'images, Traitement du signal},
}

@misc{cunningham_sparse_2023,
	title = {Sparse {Autoencoders} {Find} {Highly} {Interpretable} {Features} in {Language} {Models}},
	url = {http://arxiv.org/abs/2309.08600},
	doi = {10.48550/arXiv.2309.08600},
	abstract = {One of the roadblocks to a better understanding of neural networks' internals is {\textbackslash}textit\{polysemanticity\}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is {\textbackslash}textit\{superposition\}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task {\textbackslash}citep\{wang2022interpretability\} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
	month = oct,
	year = {2023},
	note = {arXiv:2309.08600},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/PGESICV4/Cunningham et al. - 2023 - Sparse Autoencoders Find Highly Interpretable Feat.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/VFMW5Z6Y/2309.html:text/html},
}

@misc{yun_transformer_2023,
	title = {Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors},
	shorttitle = {Transformer visualization via dictionary learning},
	url = {http://arxiv.org/abs/2103.15949},
	doi = {10.48550/arXiv.2103.15949},
	abstract = {Transformer networks have revolutionized NLP representation learning since they were introduced. Though a great effort has been made to explain the representation in transformers, it is widely recognized that our understanding is not sufficient. One important reason is that there lack enough visualization tools for detailed analysis. In this paper, we propose to use dictionary learning to open up these "black boxes" as linear superpositions of transformer factors. Through visualization, we demonstrate the hierarchical semantic structures captured by the transformer factors, e.g., word-level polysemy disambiguation, sentence-level pattern formation, and long-range dependency. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights. We hope this visualization tool can bring further knowledge and a better understanding of how transformer networks work. The code is available at https://github.com/zeyuyun1/TransformerVis},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Yun, Zeyu and Chen, Yubei and Olshausen, Bruno A. and LeCun, Yann},
	month = apr,
	year = {2023},
	note = {arXiv:2103.15949},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/KQ4CDS73/Yun et al. - 2023 - Transformer visualization via dictionary learning.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/VTIPJWXS/2103.html:text/html},
}

@misc{olah_distributed_2023,
	title = {Distributed {Representations}: {Composition} \& {Superposition}},
	url = {https://transformer-circuits.pub/2023/superposition-composition/index.html},
	urldate = {2024-11-27},
	author = {Olah, Chris},
	month = may,
	year = {2023},
	file = {Distributed Representations\: Composition & Superposition:/Users/cgadski/Zotero/storage/C6X2HACL/index.html:text/html},
}

@incollection{nadel_distributed_2006,
	edition = {1},
	title = {Distributed {Representations}},
	isbn = {978-0-470-01619-0 978-0-470-01886-6},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0470018860.s00059},
	abstract = {Abstract
            Distributed representations are a way of representing information in a pattern of activation over a set of neurons, in which each concept is represented by activation over multiple neurons, and each neuron participates in the representation of multiple concepts.},
	language = {en},
	urldate = {2024-11-27},
	booktitle = {Encyclopedia of {Cognitive} {Science}},
	publisher = {Wiley},
	author = {Plate, Tony},
	editor = {Nadel, Lynn},
	month = jan,
	year = {2006},
	doi = {10.1002/0470018860.s00059},
	file = {Available Version (via Google Scholar):/Users/cgadski/Zotero/storage/WB5XRUKZ/Plate - 2003 - Distributed representations.pdf:application/pdf},
}

@misc{elhage_toy_2022,
	title = {Toy {Models} of {Superposition}},
	url = {http://arxiv.org/abs/2209.10652},
	doi = {10.48550/arXiv.2209.10652},
	abstract = {Neural networks often pack many unrelated concepts into a single neuron - a puzzling phenomenon known as 'polysemanticity' which makes interpretability much more challenging. This paper provides a toy model where polysemanticity can be fully understood, arising as a result of models storing additional sparse features in "superposition." We demonstrate the existence of a phase change, a surprising connection to the geometry of uniform polytopes, and evidence of a link to adversarial examples. We also discuss potential implications for mechanistic interpretability.},
	urldate = {2024-11-26},
	publisher = {arXiv},
	author = {Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
	month = sep,
	year = {2022},
	note = {arXiv:2209.10652},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/EH2C8VCD/Elhage et al. - 2022 - Toy Models of Superposition.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/NH7AVZWT/2209.html:text/html},
}

@article{cammarata_curve_2020,
	title = {Curve {Detectors}},
	volume = {5},
	issn = {2476-0757},
	url = {https://distill.pub/2020/circuits/curve-detectors},
	doi = {10.23915/distill.00024.003},
	number = {6},
	urldate = {2024-11-26},
	journal = {Distill},
	author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Schubert, Ludwig and Petrov, Michael and Olah, Chris},
	month = jun,
	year = {2020},
	pages = {10.23915/distill.00024.003},
}

@article{olah_zoom_2020,
	title = {Zoom {In}: {An} {Introduction} to {Circuits}},
	volume = {5},
	issn = {2476-0757},
	shorttitle = {Zoom {In}},
	url = {https://distill.pub/2020/circuits/zoom-in},
	doi = {10.23915/distill.00024.001},
	number = {3},
	urldate = {2024-11-26},
	journal = {Distill},
	author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
	month = mar,
	year = {2020},
	pages = {10.23915/distill.00024.001},
}

@article{cammarata_thread_2020,
	title = {Thread: {Circuits}},
	volume = {5},
	issn = {2476-0757},
	shorttitle = {Thread},
	url = {https://distill.pub/2020/circuits},
	doi = {10.23915/distill.00024},
	number = {3},
	urldate = {2024-11-25},
	journal = {Distill},
	author = {Cammarata, Nick and Carter, Shan and Goh, Gabriel and Olah, Chris and Petrov, Michael and Schubert, Ludwig},
	month = mar,
	year = {2020},
	pages = {10.23915/distill.00024},
}

@book{haykin_adaptive_2013,
	address = {Upper Saddle River, NJ},
	edition = {5th edition},
	title = {Adaptive {Filter} {Theory}},
	isbn = {978-0-13-267145-3},
	abstract = {{\textless}{\textgreater} Adaptive Filter Theory, 5e, is ideal for courses in Adaptive Filters.  Haykin examines both the mathematical theory behind various linear adaptive filters and the elements of supervised multilayer perceptrons. In its fifth edition, this highly successful book has been updated and refined to stay current with the field and develop concepts in as unified and accessible a manner as possible.},
	language = {English},
	publisher = {Pearson},
	author = {Haykin, Simon O.},
	month = jun,
	year = {2013},
}

@article{cohen_compressed_2009,
	title = {Compressed sensing and best 𝑘-term approximation},
	volume = {22},
	issn = {0894-0347},
	url = {https://www.ams.org/journals/jams/2009-22-01/S0894-0347-08-00610-3/viewer/},
	number = {1},
	urldate = {2024-11-23},
	journal = {Journal of the American Mathematical Society},
	author = {Cohen, Albert and Dahmen, Wolfgang and Devore, Ronald},
	year = {2009},
	note = {Publisher: American Mathematical Society},
	pages = {211--231},
}

@misc{wattenberg_relational_2024,
	title = {Relational {Composition} in {Neural} {Networks}: {A} {Survey} and {Call} to {Action}},
	shorttitle = {Relational {Composition} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/2407.14662},
	doi = {10.48550/arXiv.2407.14662},
	abstract = {Many neural nets appear to represent data as linear combinations of "feature vectors." Algorithms for discovering these vectors have seen impressive recent success. However, we argue that this success is incomplete without an understanding of relational composition: how (or whether) neural nets combine feature vectors to represent more complicated relationships. To facilitate research in this area, this paper offers a guided tour of various relational mechanisms that have been proposed, along with preliminary analysis of how such mechanisms might affect the search for interpretable features. We end with a series of promising areas for empirical research, which may help determine how neural networks represent structured data.},
	urldate = {2024-11-22},
	publisher = {arXiv},
	author = {Wattenberg, Martin and Viégas, Fernanda B.},
	month = jul,
	year = {2024},
	note = {arXiv:2407.14662},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/D86YQCY8/Wattenberg and Viégas - 2024 - Relational Composition in Neural Networks A Surve.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/5FCJMSL7/2407.html:text/html},
}

@misc{engels_not_2024-1,
	title = {Not {All} {Language} {Model} {Features} {Are} {Linear}},
	url = {http://arxiv.org/abs/2405.14860},
	doi = {10.48550/arXiv.2405.14860},
	abstract = {Recent work has proposed that language models perform computation by manipulating one-dimensional representations of concepts ("features") in activation space. In contrast, we explore whether some language model representations may be inherently multi-dimensional. We begin by developing a rigorous definition of irreducible multi-dimensional features based on whether they can be decomposed into either independent or non-co-occurring lower-dimensional features. Motivated by these definitions, we design a scalable method that uses sparse autoencoders to automatically find multi-dimensional features in GPT-2 and Mistral 7B. These auto-discovered features include strikingly interpretable examples, e.g. circular features representing days of the week and months of the year. We identify tasks where these exact circles are used to solve computational problems involving modular arithmetic in days of the week and months of the year. Next, we provide evidence that these circular features are indeed the fundamental unit of computation in these tasks with intervention experiments on Mistral 7B and Llama 3 8B. Finally, we find further circular representations by breaking down the hidden states for these tasks into interpretable components, and we examine the continuity of the days of the week feature in Mistral 7B.},
	urldate = {2024-11-22},
	publisher = {arXiv},
	author = {Engels, Joshua and Michaud, Eric J. and Liao, Isaac and Gurnee, Wes and Tegmark, Max},
	month = oct,
	year = {2024},
	note = {arXiv:2405.14860},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/LQARG6MR/Engels et al. - 2024 - Not All Language Model Features Are Linear.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/X3WPMP9M/2405.html:text/html},
}

@misc{gorton_missing_2024,
	title = {The {Missing} {Curve} {Detectors} of {InceptionV1}: {Applying} {Sparse} {Autoencoders} to {InceptionV1} {Early} {Vision}},
	shorttitle = {The {Missing} {Curve} {Detectors} of {InceptionV1}},
	url = {http://arxiv.org/abs/2406.03662},
	doi = {10.48550/arXiv.2406.03662},
	abstract = {Recent work on sparse autoencoders (SAEs) has shown promise in extracting interpretable features from neural networks and addressing challenges with polysemantic neurons caused by superposition. In this paper, we apply SAEs to the early vision layers of InceptionV1, a well-studied convolutional neural network, with a focus on curve detectors. Our results demonstrate that SAEs can uncover new interpretable features not apparent from examining individual neurons, including additional curve detectors that fill in previous gaps. We also find that SAEs can decompose some polysemantic neurons into more monosemantic constituent features. These findings suggest SAEs are a valuable tool for understanding InceptionV1, and convolutional neural networks more generally.},
	urldate = {2024-11-22},
	publisher = {arXiv},
	author = {Gorton, Liv},
	month = sep,
	year = {2024},
	note = {arXiv:2406.03662},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/63PETEY9/Gorton - 2024 - The Missing Curve Detectors of InceptionV1 Applyi.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/M3E3IK8M/2406.html:text/html},
}

@misc{olsson_-context_2022,
	title = {In-context {Learning} and {Induction} {Heads}},
	url = {http://arxiv.org/abs/2209.11895},
	doi = {10.48550/arXiv.2209.11895},
	abstract = {"Induction heads" are attention heads that implement a simple algorithm to complete token sequences like [A][B] ... [A] -{\textgreater} [B]. In this work, we present preliminary and indirect evidence for a hypothesis that induction heads might constitute the mechanism for the majority of all "in-context learning" in large transformer models (i.e. decreasing loss at increasing token indices). We find that induction heads develop at precisely the same point as a sudden sharp increase in in-context learning ability, visible as a bump in the training loss. We present six complementary lines of evidence, arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size. For small attention-only models, we present strong, causal evidence; for larger models with MLPs, we present correlational evidence.},
	urldate = {2024-11-21},
	publisher = {arXiv},
	author = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
	month = sep,
	year = {2022},
	note = {arXiv:2209.11895},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/YUKYZFCT/Olsson et al. - 2022 - In-context Learning and Induction Heads.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/MBVW9T7D/2209.html:text/html},
}

@article{kanerva_hyperdimensional_2009,
	title = {Hyperdimensional {Computing}: {An} {Introduction} to {Computing} in {Distributed} {Representation} with {High}-{Dimensional} {Random} {Vectors}},
	volume = {1},
	issn = {1866-9964},
	shorttitle = {Hyperdimensional {Computing}},
	url = {https://doi.org/10.1007/s12559-009-9009-8},
	doi = {10.1007/s12559-009-9009-8},
	abstract = {The 1990s saw the emergence of cognitive models that depend on very high dimensionality and randomness. They include Holographic Reduced Representations, Spatter Code, Semantic Vectors, Latent Semantic Analysis, Context-Dependent Thinning, and Vector-Symbolic Architecture. They represent things in high-dimensional vectors that are manipulated by operations that produce new high-dimensional vectors in the style of traditional computing, in what is called here hyperdimensional computing on account of the very high dimensionality. The paper presents the main ideas behind these models, written as a tutorial essay in hopes of making the ideas accessible and even provocative. A sketch of how we have arrived at these models, with references and pointers to further reading, is given at the end. The thesis of the paper is that hyperdimensional representation has much to offer to students of cognitive science, theoretical neuroscience, computer science and engineering, and mathematics.},
	language = {en},
	number = {2},
	urldate = {2024-11-18},
	journal = {Cognitive Computation},
	author = {Kanerva, Pentti},
	month = jun,
	year = {2009},
	keywords = {Artificial Intelligence, Cognitive code, Holistic mapping, Holistic record, Holographic reduced representation, Quantum Computing, Random indexing, von Neumann architecture},
	pages = {139--159},
	file = {Full Text PDF:/Users/cgadski/Zotero/storage/AG7BBLE8/Kanerva - 2009 - Hyperdimensional Computing An Introduction to Com.pdf:application/pdf},
}

@misc{noauthor_distributed_2023,
	title = {Distributed {Representations}: {Composition} \& {Superposition}},
	url = {https://transformer-circuits.pub/2023/superposition-composition/index.html},
	urldate = {2024-11-12},
	month = may,
	year = {2023},
	file = {Distributed Representations\: Composition & Superposition:/Users/cgadski/Zotero/storage/I7MR3T86/index.html:text/html},
}

@misc{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	doi = {10.48550/arXiv.1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2024-11-12},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv:1301.3781},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/L4YPTXBV/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/DQS3N92Z/1301.html:text/html},
}

@misc{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	url = {http://arxiv.org/abs/1310.4546},
	doi = {10.48550/arXiv.1310.4546},
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	urldate = {2024-11-12},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = oct,
	year = {2013},
	note = {arXiv:1310.4546},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/P45BBYHH/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/J6MJGN9I/1310.html:text/html},
}

@misc{ding_survival_2024,
	title = {Survival of the {Fittest} {Representation}: {A} {Case} {Study} with {Modular} {Addition}},
	shorttitle = {Survival of the {Fittest} {Representation}},
	url = {http://arxiv.org/abs/2405.17420},
	doi = {10.48550/arXiv.2405.17420},
	abstract = {When a neural network can learn multiple distinct algorithms to solve a task, how does it "choose" between them during training? To approach this question, we take inspiration from ecology: when multiple species coexist, they eventually reach an equilibrium where some survive while others die out. Analogously, we suggest that a neural network at initialization contains many solutions (representations and algorithms), which compete with each other under pressure from resource constraints, with the "fittest" ultimately prevailing. To investigate this Survival of the Fittest hypothesis, we conduct a case study on neural networks performing modular addition, and find that these networks' multiple circular representations at different Fourier frequencies undergo such competitive dynamics, with only a few circles surviving at the end. We find that the frequencies with high initial signals and gradients, the "fittest," are more likely to survive. By increasing the embedding dimension, we also observe more surviving frequencies. Inspired by the Lotka-Volterra equations describing the dynamics between species, we find that the dynamics of the circles can be nicely characterized by a set of linear differential equations. Our results with modular addition show that it is possible to decompose complicated representations into simpler components, along with their basic interactions, to offer insight on the training dynamics of representations.},
	urldate = {2024-11-09},
	publisher = {arXiv},
	author = {Ding, Xiaoman Delores and Guo, Zifan Carl and Michaud, Eric J. and Liu, Ziming and Tegmark, Max},
	month = may,
	year = {2024},
	note = {arXiv:2405.17420},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/FYK54ZZ5/Ding et al. - 2024 - Survival of the Fittest Representation A Case Stu.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/YRMDRB79/2405.html:text/html},
}

@article{candes_decoding_2005,
	title = {Decoding by linear programming},
	volume = {51},
	issn = {1557-9654},
	url = {https://ieeexplore.ieee.org/document/1542412},
	doi = {10.1109/TIT.2005.858979},
	abstract = {This paper considers a natural error correcting problem with real valued input/output. We wish to recover an input vector f/spl isin/R/sup n/ from corrupted measurements y=Af+e. Here, A is an m by n (coding) matrix and e is an arbitrary and unknown vector of errors. Is it possible to recover f exactly from the data y? We prove that under suitable conditions on the coding matrix A, the input f is the unique solution to the /spl lscr//sub 1/-minimization problem (/spl par/x/spl par//sub /spl lscr/1/:=/spl Sigma//sub i/{\textbar}x/sub i/{\textbar}) min(g/spl isin/R/sup n/) /spl par/y - Ag/spl par//sub /spl lscr/1/ provided that the support of the vector of errors is not too large, /spl par/e/spl par//sub /spl lscr/0/:={\textbar}i:e/sub i/ /spl ne/ 0{\textbar}/spl les//spl rho//spl middot/m for some /spl rho/{\textgreater}0. In short, f can be recovered exactly by solving a simple convex optimization problem (which one can recast as a linear program). In addition, numerical experiments suggest that this recovery procedure works unreasonably well; f is recovered exactly even in situations where a significant fraction of the output is corrupted. This work is related to the problem of finding sparse solutions to vastly underdetermined systems of linear equations. There are also significant connections with the problem of recovering signals from highly incomplete measurements. In fact, the results introduced in this paper improve on our earlier work. Finally, underlying the success of /spl lscr//sub 1/ is a crucial property we call the uniform uncertainty principle that we shall describe in detail.},
	number = {12},
	urldate = {2024-11-05},
	journal = {IEEE Transactions on Information Theory},
	author = {Candes, E.J. and Tao, T.},
	month = dec,
	year = {2005},
	note = {Conference Name: IEEE Transactions on Information Theory},
	keywords = {Mathematics, Linear programming, Vectors, Basis pursuit, Decoding, decoding of (random) linear codes, duality in optimization, Equations, Error correction, Error correction codes, Gaussian random matrices, Information theory, Linear code, linear codes, linear programming, principal angles, restricted orthonormality, singular values of random matrices, Sparse matrices, sparse solutions to underdetermined systems},
	pages = {4203--4215},
	file = {IEEE Xplore Abstract Record:/Users/cgadski/Zotero/storage/CPULF3NM/1542412.html:text/html;Submitted Version:/Users/cgadski/Zotero/storage/ZVV3X2P6/Candes and Tao - 2005 - Decoding by linear programming.pdf:application/pdf},
}

@incollection{lashley_search_1950,
	address = {Oxford, England},
	title = {In search of the engram},
	abstract = {A review of the experimental evidence on neural mechanisms in learning and memory leads to conclusions: (1) "the theory of well-defined conditional reflex paths" is mistaken; (2) there is no demonstrable localization of a memory trace; (3) "associative areas" are not storehouses for memories; (4) "the trace of any activity is not an isolated connexion between sensory and motor elements"; (5) cortical equivalence indicates multiple representation of memories; (6) since all brain cells are constantly active, "no great excess of cells… can be reserved as the seat of special memories." 58 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	booktitle = {Physiological mechanisms in animal behavior. ({Society}'s {Symposium} {IV}.)},
	publisher = {Academic Press},
	author = {Lashley, K. S.},
	year = {1950},
	pages = {454--482},
	file = {Snapshot:/Users/cgadski/Zotero/storage/GZ9RKJ9C/1952-05966-020.html:text/html},
}

@article{josselyn_finding_2015,
	title = {Finding the engram},
	volume = {16},
	copyright = {2015 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/nrn4000},
	doi = {10.1038/nrn4000},
	abstract = {An engram is the physical trace of a memory in the brain. Although many attempts have been made to localize engrams, the engram has remained largely elusive until now.Here, we develop four defining criteria for engram identification and apply these criteria to recent capture studies that have attempted to observe, erase and artificially express engrams in rodents.Capture studies (allocate-and-manipulate or tag-and-manipulate) allow neurons that were active at the time of learning (engram encoding) to be captured and permanently tagged for later visualization and/or manipulation.Observation studies have established that neurons active at the time of encoding are reactivated when the corresponding memory is retrieved.Erasure studies have shown that silencing of engram neurons prevents memory expression, and thus establish that activation of these neurons is necessary for successful retrieval.Conversely, stimulation of these engram neurons has been used effectively to induce artificial memory recovery, and thus establish that activation of engram neurons is sufficient for retrieval.},
	language = {en},
	number = {9},
	urldate = {2024-11-05},
	journal = {Nature Reviews Neuroscience},
	author = {Josselyn, Sheena A. and Köhler, Stefan and Frankland, Paul W.},
	month = sep,
	year = {2015},
	note = {Publisher: Nature Publishing Group},
	keywords = {Amygdala, Fear conditioning, Hippocampus, Learning and memory},
	pages = {521--534},
	file = {Full Text PDF:/Users/cgadski/Zotero/storage/UU6K85EP/Josselyn et al. - 2015 - Finding the engram.pdf:application/pdf},
}

@inproceedings{gregor_learning_2010,
	address = {Madison, WI, USA},
	series = {{ICML}'10},
	title = {Learning fast approximations of sparse coding},
	isbn = {978-1-60558-907-7},
	abstract = {In Sparse Coding (SC), input vectors are reconstructed using a sparse linear combination of basis vectors. SC has become a popular method for extracting features from data. For a given input, SC minimizes a quadratic reconstruction error with an L1 penalty term on the code. The process is often too slow for applications such as real-time pattern recognition. We proposed two versions of a very fast algorithm that produces approximate estimates of the sparse code that can be used to compute good visual features, or to initialize exact iterative algorithms. The main idea is to train a non-linear, feed-forward predictor with a specific architecture and a fixed depth to produce the best possible approximation of the sparse code. A version of the method, which can be seen as a trainable version of Li and Osher's coordinate descent method, is shown to produce approximate solutions with 10 times less computation than Li and Os-her's for the same approximation error. Unlike previous proposals for sparse code predictors, the system allows a kind of approximate "explaining away" to take place during inference. The resulting predictor is differentiable and can be included into globally-trained recognition systems.},
	urldate = {2024-11-02},
	booktitle = {Proceedings of the 27th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Gregor, Karol and LeCun, Yann},
	month = jun,
	year = {2010},
	pages = {399--406},
}

@book{eldar_compressed_2012,
	title = {Compressed {Sensing}: {Theory} and {Applications}},
	isbn = {978-1-107-39439-1},
	shorttitle = {Compressed {Sensing}},
	abstract = {Compressed sensing is an exciting, rapidly growing field, attracting considerable attention in electrical engineering, applied mathematics, statistics and computer science. This book provides the first detailed introduction to the subject, highlighting theoretical advances and a range of applications, as well as outlining numerous remaining research challenges. After a thorough review of the basic theory, many cutting-edge techniques are presented, including advanced signal modeling, sub-Nyquist sampling of analog signals, non-asymptotic analysis of random matrices, adaptive sensing, greedy algorithms and use of graphical models. All chapters are written by leading researchers in the field, and consistent style and notation are utilized throughout. Key background information and clear definitions make this an ideal resource for researchers, graduate students and practitioners wanting to join this exciting research area. It can also serve as a supplementary textbook for courses on computer vision, coding theory, signal processing, image processing and algorithms for efficient data processing.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Eldar, Yonina C. and Kutyniok, Gitta},
	month = may,
	year = {2012},
	note = {Google-Books-ID: 9ccLAQAAQBAJ},
	keywords = {Technology \& Engineering / Electronics / General, Technology \& Engineering / Manufacturing, Technology \& Engineering / Materials Science / General, Technology \& Engineering / Mechanical, Technology \& Engineering / Signals \& Signal Processing, Technology \& Engineering / Technical \& Manufacturing Industries \& Trades, Technology \& Engineering / Telecommunications},
}

@inproceedings{ablin_learning_2019,
	title = {Learning step sizes for unfolded sparse coding},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/d073bb8d0c47f317dd39de9c9f004e9d-Abstract.html},
	abstract = {Sparse coding is typically solved by iterative optimization techniques, such as the Iterative Shrinkage-Thresholding Algorithm (ISTA). Unfolding and learning weights of ISTA using neural networks is a practical way to accelerate estimation. In this paper, we study the selection of adapted step sizes for ISTA. We show that a simple step size strategy can improve the convergence rate of ISTA by leveraging the sparsity of the iterates. However, it is impractical in most large-scale applications. Therefore, we propose a network architecture where only the step sizes of ISTA are learned. We demonstrate that for a large class of unfolded algorithms, if the algorithm converges to the solution of the Lasso, its last layers correspond to ISTA with learned step sizes. Experiments show that our method is competitive with state-of-the-art networks when the solutions are sparse enough.},
	urldate = {2024-11-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ablin, Pierre and Moreau, Thomas and Massias, Mathurin and Gramfort, Alexandre},
	year = {2019},
	file = {Full Text PDF:/Users/cgadski/Zotero/storage/GFCXSW7J/Ablin et al. - 2019 - Learning step sizes for unfolded sparse coding.pdf:application/pdf},
}

@article{thorpe_local_1989,
	title = {Local vs. {Distributed} {Coding}},
	volume = {8},
	copyright = {free},
	url = {https://www.persee.fr/doc/intel_0769-4113_1989_num_8_2_873},
	doi = {10.3406/intel.1989.873},
	abstract = {The question of how information can be represented in networks of neurons is central to cognitive science - especially so now that some researchers are turning to connectionist models in an attempt to understand human cognition. A major distinction can be made between local coding schemes, in which each unit is assigned a particular meaning, and distributed coding, in which information is encoded by large numbers of units, each of which is involved in representing many different concepts. Many brain scientists dislike local coding schemes, but it is argued here that the arguments generally invoked against local coding are far from convincing. There are situations where it has been shown that the brain does use a form of distributed coding, often referred to as coarse coding, which is particularly useful for representing continuous variables during sensory and motor processing. However, it may be appropriate to think of the brain as a hybrid system, using more local codes for higher level representation. Such a view has the advantage of allowing more conventional symbol manipulation based artificial intelligence approaches to be integrated into a connectionist framework.},
	language = {eng},
	number = {2},
	urldate = {2024-10-31},
	journal = {Intellectica},
	author = {Thorpe, Simon},
	year = {1989},
	note = {Publisher: Persée - Portail des revues scientifiques en SHS},
	pages = {3--40},
}

@misc{engels_decomposing_2024,
	title = {Decomposing {The} {Dark} {Matter} of {Sparse} {Autoencoders}},
	url = {http://arxiv.org/abs/2410.14670},
	doi = {10.48550/arXiv.2410.14670},
	abstract = {Sparse autoencoders (SAEs) are a promising technique for decomposing language model activations into interpretable linear features. However, current SAEs fall short of completely explaining model performance, resulting in "dark matter": unexplained variance in activations. This work investigates dark matter as an object of study in its own right. Surprisingly, we find that much of SAE dark matter--about half of the error vector itself and {\textgreater}90\% of its norm--can be linearly predicted from the initial activation vector. Additionally, we find that the scaling behavior of SAE error norms at a per token level is remarkably predictable: larger SAEs mostly struggle to reconstruct the same contexts as smaller SAEs. We build on the linear representation hypothesis to propose models of activations that might lead to these observations, including postulating a new type of "introduced error"; these insights imply that the part of the SAE error vector that cannot be linearly predicted ("nonlinear" error) might be fundamentally different from the linearly predictable component. To validate this hypothesis, we empirically analyze nonlinear SAE error and show that 1) it contains fewer not yet learned features, 2) SAEs trained on it are quantitatively worse, 3) it helps predict SAE per-token scaling behavior, and 4) it is responsible for a proportional amount of the downstream increase in cross entropy loss when SAE activations are inserted into the model. Finally, we examine two methods to reduce nonlinear SAE error at a fixed sparsity: inference time gradient pursuit, which leads to a very slight decrease in nonlinear error, and linear transformations from earlier layer SAE outputs, which leads to a larger reduction.},
	urldate = {2024-10-27},
	publisher = {arXiv},
	author = {Engels, Joshua and Riggs, Logan and Tegmark, Max},
	month = oct,
	year = {2024},
	note = {arXiv:2410.14670},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/GZP2SFNG/Engels et al. - 2024 - Decomposing The Dark Matter of Sparse Autoencoders.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/5LYU2R2F/2410.html:text/html},
}

@article{johnson_extensions_1984,
	title = {Extensions of {Lipschitz} maps into a {Hilbert} space},
	volume = {26},
	issn = {9780821850305},
	doi = {10.1090/conm/026/737400},
	journal = {Contemporary Mathematics},
	author = {Johnson, William and Lindenstrauss, Joram},
	month = jan,
	year = {1984},
	pages = {189--206},
	file = {Full Text PDF:/Users/cgadski/Zotero/storage/N4RZYQV5/Johnson and Lindenstrauss - 1984 - Extensions of Lipschitz maps into a Hilbert space.pdf:application/pdf},
}

@article{lashley_studies_1926,
	title = {Studies of cerebral function in learning. {VII}. {The} relation between cerebral mass, learning, and retention},
	volume = {41},
	copyright = {Copyright © 1926 The Wistar Institute of Anatomy and Biology},
	issn = {1096-9861},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cne.900410102},
	doi = {10.1002/cne.900410102},
	language = {en},
	number = {1},
	urldate = {2024-10-22},
	journal = {Journal of Comparative Neurology},
	author = {Lashley, K. S.},
	year = {1926},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cne.900410102},
	pages = {1--58},
	file = {Snapshot:/Users/cgadski/Zotero/storage/I2M8H9Y9/cne.html:text/html},
}

@misc{adler_complexity_2024,
	title = {On the {Complexity} of {Neural} {Computation} in {Superposition}},
	url = {http://arxiv.org/abs/2409.15318},
	doi = {10.48550/arXiv.2409.15318},
	abstract = {Recent advances in the understanding of neural networks suggest that superposition, the ability of a single neuron to represent multiple features simultaneously, is a key mechanism underlying the computational efficiency of large-scale networks. This paper explores the theoretical foundations of computing in superposition, focusing on explicit, provably correct algorithms and their efficiency. We present the first lower bounds showing that for a broad class of problems, including permutations and pairwise logical operations, a neural network computing in superposition requires at least \${\textbackslash}Omega(m' {\textbackslash}log m')\$ parameters and \${\textbackslash}Omega({\textbackslash}sqrt\{m' {\textbackslash}log m'\})\$ neurons, where \$m'\$ is the number of output features being computed. This implies that any ``lottery ticket'' sparse sub-network must have at least \${\textbackslash}Omega(m' {\textbackslash}log m')\$ parameters no matter what the initial dense network size. Conversely, we show a nearly tight upper bound: logical operations like pairwise AND can be computed using \$O({\textbackslash}sqrt\{m'\} {\textbackslash}log m')\$ neurons and \$O(m' {\textbackslash}log{\textasciicircum}2 m')\$ parameters. There is thus an exponential gap between computing in superposition, the subject of this work, and representing features in superposition, which can require as little as \$O({\textbackslash}log m'\$) neurons based on the Johnson-Lindenstrauss Lemma. Our hope is that our results open a path for using complexity theoretic techniques in neural network interpretability research.},
	urldate = {2024-10-20},
	publisher = {arXiv},
	author = {Adler, Micah and Shavit, Nir},
	month = sep,
	year = {2024},
	note = {arXiv:2409.15318},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Computer Science - Data Structures and Algorithms, Computer Science - Computational Complexity},
	file = {Full Text:/Users/cgadski/Zotero/storage/73QMV3SI/Adler and Shavit - 2024 - On the Complexity of Neural Computation in Superpo.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/8GMYBRLX/2409.html:text/html},
}

@misc{zou_concise_2022,
	title = {A {Concise} {Tutorial} on {Approximate} {Message} {Passing}},
	url = {http://arxiv.org/abs/2201.07487},
	doi = {10.48550/arXiv.2201.07487},
	abstract = {High-dimensional signal recovery of standard linear regression is a key challenge in many engineering fields, such as, communications, compressed sensing, and image processing. The approximate message passing (AMP) algorithm proposed by Donoho {\textbackslash}textit\{et al\} is a computational efficient method to such problems, which can attain Bayes-optimal performance in independent identical distributed (IID) sub-Gaussian random matrices region. A significant feature of AMP is that the dynamical behavior of AMP can be fully predicted by a scalar equation termed station evolution (SE). Although AMP is optimal in IID sub-Gaussian random matrices, AMP may fail to converge when measurement matrix is beyond IID sub-Gaussian. To extend the region of random measurement matrix, an expectation propagation (EP)-related algorithm orthogonal AMP (OAMP) was proposed, which shares the same algorithm with EP, expectation consistent (EC), and vector AMP (VAMP). This paper aims at giving a review for those algorithms. We begin with the worst case, i.e., least absolute shrinkage and selection operator (LASSO) inference problem, and then give the detailed derivation of AMP derived from message passing. Also, in the Bayes-optimal setting, we give the Bayes-optimal AMP which has a slight difference from AMP for LASSO. In addition, we review some AMP-related algorithms: OAMP, VAMP, and Memory AMP (MAMP), which can be applied to more general random matrices.},
	urldate = {2024-09-18},
	publisher = {arXiv},
	author = {Zou, Qiuyun and Yang, Hongwen},
	month = feb,
	year = {2022},
	note = {arXiv:2201.07487 [cs, math]},
	keywords = {Computer Science - Information Theory},
	file = {arXiv Fulltext PDF:/Users/cgadski/Zotero/storage/EKNZYSJU/Zou and Yang - 2022 - A Concise Tutorial on Approximate Message Passing.pdf:application/pdf;arXiv.org Snapshot:/Users/cgadski/Zotero/storage/56E4ZRJS/2201.html:text/html},
}

@misc{nanda_emergent_2023,
	title = {Emergent {Linear} {Representations} in {World} {Models} of {Self}-{Supervised} {Sequence} {Models}},
	url = {http://arxiv.org/abs/2309.00941},
	doi = {10.48550/arXiv.2309.00941},
	abstract = {How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state (Li et al., 2023). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for "my colour" vs. "opponent's colour" may be a simple yet powerful way to interpret the model's internal state. This precise understanding of the internal representations allows us to control the model's behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed.},
	urldate = {2024-09-09},
	publisher = {arXiv},
	author = {Nanda, Neel and Lee, Andrew and Wattenberg, Martin},
	month = sep,
	year = {2023},
	note = {arXiv:2309.00941 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/cgadski/Zotero/storage/CCVUNXDV/Nanda et al. - 2023 - Emergent Linear Representations in World Models of.pdf:application/pdf;arXiv.org Snapshot:/Users/cgadski/Zotero/storage/WP29R4WH/2309.html:text/html},
}

@article{calderbank_compressed_2009,
	title = {Compressed learning: {Universal} sparse dimensionality reduction and learning in the measurement domain},
	shorttitle = {Compressed learning},
	url = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=627c14fe9097d459b8fd47e8a901694198be9d5d},
	urldate = {2024-09-05},
	journal = {preprint},
	author = {Calderbank, Robert and Jafarpour, Sina and Schapire, Robert},
	year = {2009},
	note = {Publisher: Citeseer},
	file = {Available Version (via Google Scholar):/Users/cgadski/Zotero/storage/45ZRH4RF/Calderbank et al. - 2009 - Compressed learning Universal sparse dimensionali.pdf:application/pdf},
}

@article{donoho_sparse_2005,
	title = {Sparse nonnegative solution of underdetermined linear equations by linear programming},
	volume = {102},
	doi = {10.1073/pnas.0502269102},
	abstract = {Consider an underdetermined system of linear equations y = Ax with known y and d × n matrix A. We seek the nonnegative x with the fewest nonzeros satisfying y = Ax. In general, this problem is NP-hard. However, for many matrices A there is a threshold phenomenon: if the sparsest solution is sufficiently sparse, it can be found by linear programming. We explain this by the theory of convex polytopes. Let aj denote the jth column of A, 1 ≤ j ≤ n, let a 0 = 0 and P denote the convex hull of the aj. We say the polytope P is outwardly k-neighborly if every subset of k vertices not including 0 spans a face of P. We show that outward k-neighborliness is equivalent to the statement that, whenever y = Ax has a nonnegative solution with at most k nonzeros, it is the nonnegative solution to y = Ax having minimal sum. We also consider weak neighborliness, where the overwhelming majority of k-sets of ajs not containing 0 span a face of P. This implies that most nonnegative vectors x with k nonzeros are uniquely recoverable from y = Ax by linear programming. Numerous corollaries follow by invoking neighborliness results. For example, for most large n by 2n underdetermined systems having a solution with fewer nonzeros than roughly half the number of equations, the sparsest solution can be found by linear programming.

• neighborly polytopes
• cyclic polytopes
• combinatorial optimization
• convex hull of Gaussian samples
• positivity constraints in ill-posed problems},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Donoho, David and Tanner, Jared},
	month = aug,
	year = {2005},
	pages = {9446--51},
	file = {Full Text PDF:/Users/cgadski/Zotero/storage/KM57YIZL/Donoho and Tanner - 2005 - Sparse nonnegative solution of underdetermined lin.pdf:application/pdf},
}

@misc{trenton_bricken_towards_2023,
	title = {Towards {Monosemanticity}: {Decomposing} {Language} {Models} {With} {Dictionary} {Learning}},
	url = {https://transformer-circuits.pub/2023/monosemantic-features/index.html},
	urldate = {2024-07-23},
	author = {{Trenton Bricken} and {Adly Templeton} and {Joshua Batson} and {Brian Chen} and {Adam Jermyn}},
	month = oct,
	year = {2023},
}

@misc{gao_scaling_2024,
	title = {Scaling and evaluating sparse autoencoders},
	url = {http://arxiv.org/abs/2406.04093},
	doi = {10.48550/arXiv.2406.04093},
	abstract = {Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried. Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity. We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size. To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. We release training code and autoencoders for open-source models, as well as a visualizer.},
	urldate = {2024-07-23},
	publisher = {arXiv},
	author = {Gao, Leo and la Tour, Tom Dupré and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey},
	month = jun,
	year = {2024},
	note = {arXiv:2406.04093 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/9BNJV4GK/Gao et al. - 2024 - Scaling and evaluating sparse autoencoders.pdf:application/pdf},
}

@inproceedings{arora_compressed_2018,
	title = {A {Compressed} {Sensing} {View} of {Unsupervised} {Text} {Embeddings}, {Bag}-of-n-{Grams}, and {LSTMs}},
	url = {https://openreview.net/forum?id=B1e5ef-C-},
	abstract = {Low-dimensional vector embeddings, computed using LSTMs or simpler techniques, are a popular approach for capturing the “meaning” of text and a form of unsupervised learning useful for downstream tasks. However, their power is not theoretically understood. The current paper derives formal understanding by looking at the subcase of linear embedding schemes. Using the theory of compressed sensing we show that representations combining the constituent word vectors are essentially information-preserving linear measurements of Bag-of-n-Grams (BonG) representations of text. This leads to a new theoretical result about LSTMs: low-dimensional embeddings derived from a low-memory LSTM are provably at least as powerful on classification tasks, up to small error, as a linear classifier over BonG vectors, a result that extensive empirical work has thus far been unable to show. Our experiments support these theoretical findings and establish strong, simple, and unsupervised baselines on standard benchmarks that in some cases are state of the art among word-level methods. We also show a surprising new property of embeddings such as GloVe and word2vec: they form a good sensing matrix for text that is more efficient than random matrices, the standard sparse recovery tool, which may explain why they lead to better representations in practice.},
	language = {en},
	urldate = {2024-06-24},
	author = {Arora, Sanjeev and Khodak, Mikhail and Saunshi, Nikunj and Vodrahalli, Kiran},
	month = feb,
	year = {2018},
	file = {Full Text PDF:/Users/cgadski/Zotero/storage/AFZRQE4W/Arora et al. - 2018 - A Compressed Sensing View of Unsupervised Text Emb.pdf:application/pdf},
}

@article{dasgupta_elementary_2003,
	title = {An elementary proof of a theorem of {Johnson} and {Lindenstrauss}},
	volume = {22},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	issn = {1042-9832, 1098-2418},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/rsa.10073},
	doi = {10.1002/rsa.10073},
	abstract = {Abstract
            
              A result of Johnson and Lindenstrauss [13] shows that a set of
              n
              points in high dimensional Euclidean space can be mapped into an
              O(
              log
              n/ϵ
              
                2
              
              )‐dimensional Euclidean space such that the distance between any two points changes by only a factor of (1 ± ϵ). In this note, we prove this theorem using elementary probabilistic techniques. © 2002 Wiley Periodicals, Inc. Random Struct. Alg., 22: 60–65, 2002},
	language = {en},
	number = {1},
	urldate = {2024-06-23},
	journal = {Random Structures \& Algorithms},
	author = {Dasgupta, Sanjoy and Gupta, Anupam},
	month = jan,
	year = {2003},
	pages = {60--65},
	file = {Dasgupta and Gupta - 2003 - An elementary proof of a theorem of Johnson and Li.pdf:/Users/cgadski/Zotero/storage/E7Z8GQZE/Dasgupta and Gupta - 2003 - An elementary proof of a theorem of Johnson and Li.pdf:application/pdf},
}

@misc{donoho_message_2009,
	title = {Message {Passing} {Algorithms} for {Compressed} {Sensing}},
	url = {http://arxiv.org/abs/0907.3574},
	doi = {10.1073/pnas.0909892106},
	abstract = {Compressed sensing aims to undersample certain high-dimensional signals, yet accurately reconstruct them by exploiting signal characteristics. Accurate reconstruction is possible when the object to be recovered is sufficiently sparse in a known basis. Currently, the best known sparsity-undersampling tradeoff is achieved when reconstructing by convex optimization -- which is expensive in important large-scale applications. Fast iterative thresholding algorithms have been intensively studied as alternatives to convex optimization for large-scale problems. Unfortunately known fast algorithms offer substantially worse sparsity-undersampling tradeoffs than convex optimization. We introduce a simple costless modification to iterative thresholding making the sparsity-undersampling tradeoff of the new algorithms equivalent to that of the corresponding convex optimization procedures. The new iterative-thresholding algorithms are inspired by belief propagation in graphical models. Our empirical measurements of the sparsity-undersampling tradeoff for the new algorithms agree with theoretical calculations. We show that a state evolution formalism correctly derives the true sparsity-undersampling tradeoff. There is a surprising agreement between earlier calculations based on random convex polytopes and this new, apparently very different theoretical formalism.},
	urldate = {2024-05-29},
	author = {Donoho, David L. and Maleki, Arian and Montanari, Andrea},
	month = jul,
	year = {2009},
	note = {arXiv:0907.3574 [cond-mat, stat]},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Statistics - Computation, Computer Science - Information Theory},
	file = {arXiv Fulltext PDF:/Users/cgadski/Zotero/storage/WX8C9F2J/Donoho et al. - 2009 - Message Passing Algorithms for Compressed Sensing.pdf:application/pdf;arXiv.org Snapshot:/Users/cgadski/Zotero/storage/KM7KSURW/0907.html:text/html},
}

@misc{donoho_counting_2006,
	title = {Counting faces of randomly-projected polytopes when the projection radically lowers dimension},
	url = {http://arxiv.org/abs/math/0607364},
	doi = {10.48550/arXiv.math/0607364},
	abstract = {This paper develops asymptotic methods to count faces of random high-dimensional polytopes. Beyond its intrinsic interest, our conclusions have surprising implications - in statistics, probability, information theory, and signal processing - with potential impacts in practical subjects like medical imaging and digital communications. Three such implications concern: convex hulls of Gaussian point clouds, signal recovery from random projections, and how many gross errors can be efficiently corrected from Gaussian error correcting codes.},
	urldate = {2025-08-01},
	publisher = {arXiv},
	author = {Donoho, David L. and Tanner, Jared},
	month = sep,
	year = {2006},
	note = {arXiv:math/0607364},
	keywords = {Mathematics - Probability, Mathematics - Statistics Theory, Mathematics - Numerical Analysis, Mathematics - Metric Geometry, Statistics - Statistics Theory},
	file = {Preprint PDF:/Users/cgadski/Zotero/storage/6EPANYTJ/Donoho and Tanner - 2006 - Counting faces of randomly-projected polytopes when the projection radically lowers dimension.pdf:application/pdf;Snapshot:/Users/cgadski/Zotero/storage/IL8Z5A6W/0607364.html:text/html},
}

@article{donoho_counting_2009,
	title = {Counting {Faces} of {Randomly} {Projected} {Polytopes} {When} the {Projection} {Radically} {Lowers} {Dimension}},
	volume = {22},
	issn = {0894-0347},
	url = {https://www.jstor.org/stable/40587224},
	number = {1},
	urldate = {2025-10-19},
	journal = {Journal of the American Mathematical Society},
	author = {Donoho, David L. and Tanner, Jared},
	year = {2009},
	note = {Publisher: American Mathematical Society},
	pages = {1--53},
}

@incollection{pfander_gabor_2013,
	address = {Boston},
	title = {Gabor {Frames} in {Finite} {Dimensions}},
	isbn = {978-0-8176-8373-3},
	url = {https://doi.org/10.1007/978-0-8176-8373-3_6},
	abstract = {Gabor frames have been extensively studied in time-frequency analysis over the last 30 years. They are commonly used in science and engineering to synthesize signals from, or to decompose signals into, building blocks which are localized in time and frequency. This chapter contains a basic and self-contained introduction to Gabor frames on finite-dimensional complex vector spaces. In this setting, we give elementary proofs of the central results on Gabor frames in the greatest possible generality; that is, we consider Gabor frames corresponding to lattices in arbitrary finite Abelian groups. In the second half of this chapter, we review recent results on the geometry of Gabor systems in finite dimensions: the linear independence of subsets of its members, their mutual coherence, and the restricted isometry property for such systems. We apply these results to the recovery of sparse signals, and discuss open questions on the geometry of finite-dimensional Gabor systems.},
	language = {en},
	urldate = {2025-09-20},
	booktitle = {Finite {Frames}: {Theory} and {Applications}},
	publisher = {Birkhäuser},
	author = {Pfander, Götz E.},
	editor = {Casazza, Peter G. and Kutyniok, Gitta},
	year = {2013},
	doi = {10.1007/978-0-8176-8373-3_6},
	keywords = {Applications to compressed sensing, Channel identification, Coherence, Erasure channel error correction, Gabor analysis on finite Abelian groups, Linear independence, Restricted isometry constants of Gabor frames},
	pages = {193--239},
	file = {Full Text PDF:/Users/cgadski/Zotero/storage/EXRH775I/Pfander - 2013 - Gabor Frames in Finite Dimensions.pdf:application/pdf},
}

@book{cover_elements_2006,
	address = {Hoboken, N.J},
	title = {Elements of {Information} {Theory} 2nd {Edition}},
	isbn = {978-0-471-24195-9},
	abstract = {The latest edition of this classic is updated with new problem sets and materialThe Second Edition of this fundamental textbook maintains the book's tradition of clear, thought-provoking instruction. Readers are provided once again with an instructive mix of mathematics, physics, statistics, and information theory.All the essential topics in information theory are covered in detail, including entropy, data compression, channel capacity, rate distortion, network information theory, and hypothesis testing. The authors provide readers with a solid understanding of the underlying theory and applications. Problem sets and a telegraphic summary at the end of each chapter further assist readers. The historical notes that follow each chapter recap the main points.The Second Edition features:Chapters reorganized to improve teaching200 new problemsNew material on source coding, portfolio theory, and feedback capacityUpdated referencesNow current and enhanced, the Second Edition of Elements of Information Theory remains the ideal textbook for upper-level undergraduate and graduate courses in electrical engineering, statistics, and telecommunications.},
	language = {English},
	publisher = {Wiley-Interscience},
	author = {Cover, Thomas M. and Thomas, Joy A.},
	year = {2006},
}
