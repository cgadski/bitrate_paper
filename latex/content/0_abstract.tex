\begin{abstract}
Recent work in neural network interpretability has suggested that hidden activations of some deep models can be viewed as linear projections of much higher-dimensional vectors of sparse latent ``features.'' In general, this kind of representation is known as a superposition code. This work presents an information-theoretic account of superposition codes in a setting applicable to interpretability. We show that, when the number $k$ of active features is very small compared to the number $N$ of total possible features, surprisingly simple methods can decode these representations provided that $d$ is a constant factor greater than the Shannon limit. Specifically, when $\ln k / \ln N \le \eta < 1$ and $H$ is the entropy of the latent vector, it suffices that $d / H \ge C(\eta)$ for a certain increasing function $C(\eta).$ However, the maximum value of $d/H$ varies significantly depending on the decoding method used. For example, the popular ``top-$k$'' method typically requires more than $4$ dimensions per bit. On the other hand, we show empirically that a family of cheap iterative methods can achieve around $3/4$ bits per dimension. Our results address a lack of practical references on superposition codes in a very sparse regime.
\end{abstract}
