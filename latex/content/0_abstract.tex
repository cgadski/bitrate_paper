\begin{abstract}
Recent work in neural network interpretability has posited that hidden activations of some deep models can be viewed as superpositions of sparsely activated ``feature vectors.'' In general, this kind of representation is known as a superposition code. This work presents an information-theoretic account of superposition codes in a setting intended to model applications of SAEs. We show that, when the number $k$ of active features is very small compared to the number $N$ of total possible features, surprisingly simple superposition methods can encode and decode these representations provided that $d$ is a constant factor greater than the Shannon limit. Specifically, when $\ln k / \ln N \le \epsilon < 1,$ it suffices that $C(\eta) d \ge H,$ where $H$ is the entropy to be transmitted and $C(\eta)$ is a certain decreasing function of $\eta.$. However, the maximum value of $d/H$ varies significantly depending on the decoding method used. For example, under moderate values of $\eta,$ we find that certain natural decoding methods are limited to transmission rates of less than $1/6$ bits per dimension. On the other hand, we show empirically that a cheap iterative method can achieve around $3/4$ bits per dimension. Our results address a lack of practical references on superposition codes in a very sparse regime and provide a possible information-theoretic explanation for the limited success of SAEs.
\end{abstract}
